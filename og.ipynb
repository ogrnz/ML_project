{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7480076e",
   "metadata": {},
   "source": [
    "## ML competition\n",
    "\n",
    "_Marilyn, Shiva, Olivier_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db1a3477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup chunk\n",
    "\n",
    "import time\n",
    "\n",
    "# Data wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Text sanitization\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "try:\n",
    "    # Avoid error if you don't have the resource\n",
    "    stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "except LookupError:\n",
    "    nltk.download(\"stopwords\")\n",
    "    stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "    \n",
    "stemmer = SnowballStemmer(language=\"english\")\n",
    "\n",
    "# Lang detection\n",
    "import langid\n",
    "from langid.langid import LanguageIdentifier, model\n",
    "identifier = LanguageIdentifier.from_modelstring(model, norm_probs=True)\n",
    "\n",
    "# Misc\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# Define the seed for reproducibility\n",
    "SEED = 31415"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a088a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit time\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import (\n",
    "    LinearDiscriminantAnalysis,\n",
    "    QuadraticDiscriminantAnalysis,\n",
    ")\n",
    "\n",
    "from sklearn.feature_extraction.text import (\n",
    "    CountVectorizer, \n",
    "    TfidfTransformer, \n",
    "    TfidfVectorizer\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, \n",
    "    GridSearchCV, \n",
    "    KFold, \n",
    "    cross_val_score\n",
    ")\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    accuracy_score, \n",
    "    confusion_matrix\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17c7404c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/MLUnige2021_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5661f6ae",
   "metadata": {},
   "source": [
    "### 1. EDA\n",
    "Small EDA to check a bit the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbf60c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1280000 entries, 0 to 1279999\n",
      "Data columns (total 7 columns):\n",
      " #   Column     Non-Null Count    Dtype \n",
      "---  ------     --------------    ----- \n",
      " 0   Id         1280000 non-null  int64 \n",
      " 1   emotion    1280000 non-null  int64 \n",
      " 2   tweet_id   1280000 non-null  int64 \n",
      " 3   date       1280000 non-null  object\n",
      " 4   lyx_query  1280000 non-null  object\n",
      " 5   user       1280000 non-null  object\n",
      " 6   text       1280000 non-null  object\n",
      "dtypes: int64(3), object(4)\n",
      "memory usage: 68.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd0f371",
   "metadata": {},
   "source": [
    "More than a million entries. What is `lyx_query`? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a33f509c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NO_QUERY'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"lyx_query\"].head()\n",
    "df[\"lyx_query\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e39a44",
   "metadata": {},
   "source": [
    "Welp only `\"NO_QUERY\"` so we can drop it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07265595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          2063391019\n",
       "1          2000525676\n",
       "2          2218180611\n",
       "3          2190269101\n",
       "4          2069249490\n",
       "              ...    \n",
       "1279995    1835296397\n",
       "1279996    2226720395\n",
       "1279997    1962176213\n",
       "1279998    1976894947\n",
       "1279999    1563596981\n",
       "Name: tweet_id, Length: 1280000, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"tweet_id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c428a98",
   "metadata": {},
   "source": [
    "Those are old tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6ff2146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          Sun Jun 07 02:28:13 PDT 2009\n",
       "1          Mon Jun 01 22:18:53 PDT 2009\n",
       "2          Wed Jun 17 22:01:38 PDT 2009\n",
       "3          Tue Jun 16 02:14:47 PDT 2009\n",
       "4          Sun Jun 07 15:31:58 PDT 2009\n",
       "                       ...             \n",
       "1279995    Mon May 18 05:39:18 PDT 2009\n",
       "1279996    Thu Jun 18 12:18:05 PDT 2009\n",
       "1279997    Fri May 29 10:38:30 PDT 2009\n",
       "1279998    Sat May 30 19:28:13 PDT 2009\n",
       "1279999    Sun Apr 19 23:27:25 PDT 2009\n",
       "Name: date, Length: 1280000, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"date\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b559dec2",
   "metadata": {},
   "source": [
    "Indeed they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "272b6252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "574114"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[\"user\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f04997c",
   "metadata": {},
   "source": [
    "Lots of different users. If we had only like 1000s of users, we could have looked for some pattern (user X is only positive ...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32ca6c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emotion                                                    1\n",
       "text       @BreeMe more time to play with you BlackBerry ...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0, [\"emotion\", \"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12804dd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"emotion\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9599f7eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "236"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"emotion\"]==1].shape[0] - df[df[\"emotion\"]==0].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2903056c",
   "metadata": {},
   "source": [
    "Perfectly balanced dataset (236 diff between the 2 classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8580f41e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.bar([\"negative\", \"positive\"], [df[df[\"emotion\"]==0].shape[0], df[df[\"emotion\"]==1].shape[0]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509f8763",
   "metadata": {},
   "source": [
    "Let's also check the language of the tweets (all eng or also others?). For that, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8377ffa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lang_detect(txt, threshold=0.95):\n",
    "    \"\"\"\n",
    "    Detect tweet language\n",
    "    returns None if confidence lvl < threshold\n",
    "    \"\"\"\n",
    "\n",
    "    if txt is None:\n",
    "        return None\n",
    "\n",
    "    txt = txt.replace(\"\\n\", \" \")\n",
    "    lang = identifier.classify(txt)\n",
    "    if lang[1] < threshold:\n",
    "        return None\n",
    "    else:\n",
    "        return lang[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c093c5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_samp = df.sample(150_000) #Total df takes 1h to run\n",
    "\n",
    "# Commented because we don't want it to run everytime\n",
    "#df_samp[\"lang\"] = df_samp[\"text\"].progress_apply(lang_detect)\n",
    "#df_samp[\"lang\"].unique()\n",
    "#df_samp[~(df_samp[\"lang\"] == \"en\")].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37af334d",
   "metadata": {},
   "source": [
    "Only 15k tweets which are not detected as english in our 150k sample. By checking some of the tweets, most are english, but the language detector surely has some trouble with some very short tweets containing one or more foreign words.--> consider all as english!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a8e276",
   "metadata": {},
   "source": [
    "## 2. Strategy\n",
    "\n",
    "1. Preprocess the text\n",
    "    1. remove punctuation marks\n",
    "    2. remove stopwords (en)\n",
    "    3. stem or lemmatize the words\n",
    "    \n",
    "    \n",
    "2. take a sample of our whole dataset (200k?) to do our preliminary test. We can't do cross validation on the whole dataset.\n",
    "\n",
    "3. Begin to fit the models. \n",
    "    1. Pipeline with TfidfTransformer (or the other one I don't remember the name)\n",
    "    2. BerNB\n",
    "    3. LogisticRegression\n",
    "    4. RidgeClassifier\n",
    "    5. SGDClassifier\n",
    "    6. SVC\n",
    "    7. RandomForestClassifier\n",
    "    8. DecisionTreeClassifier\n",
    "    9. KNeighborsClassifier\n",
    "    10. LDA? QDA?\n",
    "4. Select a few of the best models, CV with bigger dataset\n",
    "5. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3e7e6f",
   "metadata": {},
   "source": [
    "## Misc\n",
    "\n",
    "- Should pay attention to some special chars which were not removed: \"I \\&lt;3 you\" (#2204), at first just remove it, but we could try investigate that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "f16fe1b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7082"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"text\"].str.contains(\"&lt;3\")].count()\n",
    "(df[df[\"text\"].str.contains(\"&lt;3\")].loc[:, \"emotion\"] == 1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4d04a3",
   "metadata": {},
   "source": [
    "10635 tweets have that, don't think it's worth it. And out of those, only 7082 are positive. It's not even as sure as that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aff99ee",
   "metadata": {},
   "source": [
    "# 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bedbb2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize(text: str) -> str:\n",
    "    \"\"\"Sanitize a string.\"\"\"\n",
    "    \n",
    "    # Edited regex from @gruber\n",
    "    # https://gist.github.com/gruber/8891611\n",
    "    url_re = r\"\"\"(?i)\\b((?:https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\-]+[.](?:com|net|org|edu|gov)/)(?:[^\\s()<>{}\\[\\]]+|\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\))+(?:\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’])|(?:(?<!@)[a-z0-9]+(?:[.\\-][a-z0-9]+)*[.](?:com|net|org|edu|gov)\\b/?(?!@)))\"\"\"\n",
    "    \n",
    "    edited_text = re.sub(\n",
    "        \"  \", \" \", text\n",
    "    )  # replace double whitespace with single whitespace\n",
    "    edited_text = re.sub(\"@(?=.*\\w)[\\w]{1,15}\", \"\", edited_text) # remove twitter handle\n",
    "    edited_text = re.sub(\n",
    "        url_re, \"\", edited_text\n",
    "    )  # remove URL\n",
    "    edited_text = edited_text.split(\" \")  # split the sentence into array of strs\n",
    "    edited_text = \" \".join(\n",
    "        [char for char in edited_text if char != \"\"]\n",
    "    )  # remove any empty str from text\n",
    "    edited_text = edited_text.lower()  # lowercase\n",
    "    edited_text = re.sub(\"\\d+\", \"\", edited_text)  # Removing numerics\n",
    "    edited_text = re.split(\n",
    "        \"\\W+\", edited_text\n",
    "    )  # spliting based on whitespace or whitespaces\n",
    "    edited_text = \" \".join(\n",
    "        [stemmer.stem(word) for word in edited_text if word not in stopwords]\n",
    "    )  # Snowball Stemmer\n",
    "    \n",
    "    return edited_text\n",
    "\n",
    "# Some special strings to test \n",
    "txt1 = \"Hello @Shiva and @Marilyn!\"\n",
    "#print(\"Sanitized:\", sanitize(txt1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "241fa4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 150000/150000 [00:17<00:00, 8453.81it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>emotion</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>date</th>\n",
       "      <th>lyx_query</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>sanitized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1198382</th>\n",
       "      <td>1198382</td>\n",
       "      <td>0</td>\n",
       "      <td>2326120375</td>\n",
       "      <td>Thu Jun 25 06:34:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>YagerGirl4life</td>\n",
       "      <td>Has just about had it with this damn headache!</td>\n",
       "      <td>damn headach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1211635</th>\n",
       "      <td>1211635</td>\n",
       "      <td>0</td>\n",
       "      <td>2055283388</td>\n",
       "      <td>Sat Jun 06 09:02:10 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>f33dyourlungs_</td>\n",
       "      <td>@davashmava: I'd tell you, but then I'd have t...</td>\n",
       "      <td>tell kill gt buuuut tell person laugh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168420</th>\n",
       "      <td>1168420</td>\n",
       "      <td>1</td>\n",
       "      <td>1991932318</td>\n",
       "      <td>Mon Jun 01 07:55:01 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>MissSimone32</td>\n",
       "      <td>Lmao Basics  @MissPrecious2</td>\n",
       "      <td>lmao basic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751176</th>\n",
       "      <td>751176</td>\n",
       "      <td>0</td>\n",
       "      <td>1881356648</td>\n",
       "      <td>Fri May 22 04:25:21 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mabelmusic</td>\n",
       "      <td>bored. haven't anything to do.. everyone's gon...</td>\n",
       "      <td>bore anyth everyon gone tonight go great movi ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1124769</th>\n",
       "      <td>1124769</td>\n",
       "      <td>0</td>\n",
       "      <td>1694142943</td>\n",
       "      <td>Mon May 04 01:16:52 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tsyatsya</td>\n",
       "      <td>finished 5th on Qualifier ... Went &amp;quot;Meh&amp;q...</td>\n",
       "      <td>finish th qualifi went quot meh quot mirror aj...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Id  emotion    tweet_id                          date lyx_query  \\\n",
       "1198382  1198382        0  2326120375  Thu Jun 25 06:34:53 PDT 2009  NO_QUERY   \n",
       "1211635  1211635        0  2055283388  Sat Jun 06 09:02:10 PDT 2009  NO_QUERY   \n",
       "1168420  1168420        1  1991932318  Mon Jun 01 07:55:01 PDT 2009  NO_QUERY   \n",
       "751176    751176        0  1881356648  Fri May 22 04:25:21 PDT 2009  NO_QUERY   \n",
       "1124769  1124769        0  1694142943  Mon May 04 01:16:52 PDT 2009  NO_QUERY   \n",
       "\n",
       "                   user                                               text  \\\n",
       "1198382  YagerGirl4life    Has just about had it with this damn headache!    \n",
       "1211635  f33dyourlungs_  @davashmava: I'd tell you, but then I'd have t...   \n",
       "1168420    MissSimone32                        Lmao Basics  @MissPrecious2   \n",
       "751176       mabelmusic  bored. haven't anything to do.. everyone's gon...   \n",
       "1124769        tsyatsya  finished 5th on Qualifier ... Went &quot;Meh&q...   \n",
       "\n",
       "                                                 sanitized  \n",
       "1198382                                      damn headach   \n",
       "1211635              tell kill gt buuuut tell person laugh  \n",
       "1168420                                         lmao basic  \n",
       "751176   bore anyth everyon gone tonight go great movi ...  \n",
       "1124769  finish th qualifi went quot meh quot mirror aj...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As a test, sanitize the subsample\n",
    "df_samp[\"sanitized\"] = df_samp[\"text\"].progress_apply(sanitize)\n",
    "df_samp.head() #seems good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28138453",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_san = pd.read_pickle(\"./data/sanitized.pkl\")\n",
    "except FileNotFoundError:\n",
    "    print(\"No pickle file found, sanitizing existing df\")\n",
    "    \n",
    "    # Sanitize whole dataset\n",
    "    df_san = df.copy()\n",
    "    df_san[\"sanitized\"] = df[\"text\"].progress_apply(sanitize)\n",
    "\n",
    "    # Export it to pickle so we don't have to redo it\n",
    "    df_san.to_pickle(\"./data/sanitized.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33620a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1280000, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>emotion</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>date</th>\n",
       "      <th>lyx_query</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>sanitized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2063391019</td>\n",
       "      <td>Sun Jun 07 02:28:13 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>BerryGurus</td>\n",
       "      <td>@BreeMe more time to play with you BlackBerry ...</td>\n",
       "      <td>time play blackberri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2000525676</td>\n",
       "      <td>Mon Jun 01 22:18:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>peterlanoie</td>\n",
       "      <td>Failed attempt at booting to a flash drive. Th...</td>\n",
       "      <td>fail attempt boot flash drive fail attempt swi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2218180611</td>\n",
       "      <td>Wed Jun 17 22:01:38 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>will_tooker</td>\n",
       "      <td>@msproductions Well ain't that the truth. Wher...</td>\n",
       "      <td>well truth damn auto lock disabl go copi past ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2190269101</td>\n",
       "      <td>Tue Jun 16 02:14:47 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>sammutimer</td>\n",
       "      <td>@Meaghery cheers Craig - that was really sweet...</td>\n",
       "      <td>cheer craig realli sweet repli pump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2069249490</td>\n",
       "      <td>Sun Jun 07 15:31:58 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ohaijustin</td>\n",
       "      <td>I was reading the tweets that got send to me w...</td>\n",
       "      <td>read tweet got send lie phone face drop amp hi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  emotion    tweet_id                          date lyx_query  \\\n",
       "0   0        1  2063391019  Sun Jun 07 02:28:13 PDT 2009  NO_QUERY   \n",
       "1   1        0  2000525676  Mon Jun 01 22:18:53 PDT 2009  NO_QUERY   \n",
       "2   2        0  2218180611  Wed Jun 17 22:01:38 PDT 2009  NO_QUERY   \n",
       "3   3        1  2190269101  Tue Jun 16 02:14:47 PDT 2009  NO_QUERY   \n",
       "4   4        0  2069249490  Sun Jun 07 15:31:58 PDT 2009  NO_QUERY   \n",
       "\n",
       "          user                                               text  \\\n",
       "0   BerryGurus  @BreeMe more time to play with you BlackBerry ...   \n",
       "1  peterlanoie  Failed attempt at booting to a flash drive. Th...   \n",
       "2  will_tooker  @msproductions Well ain't that the truth. Wher...   \n",
       "3   sammutimer  @Meaghery cheers Craig - that was really sweet...   \n",
       "4   ohaijustin  I was reading the tweets that got send to me w...   \n",
       "\n",
       "                                           sanitized  \n",
       "0                               time play blackberri  \n",
       "1  fail attempt boot flash drive fail attempt swi...  \n",
       "2  well truth damn auto lock disabl go copi past ...  \n",
       "3               cheer craig realli sweet repli pump   \n",
       "4  read tweet got send lie phone face drop amp hi...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_san.shape)\n",
    "df_san.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be857543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before sanitizing 18398298\n",
      "After sanitizing 9575942\n"
     ]
    }
   ],
   "source": [
    "print(\"Before sanitizing\", df['text'].apply(lambda x: len(x.split(' '))).sum())\n",
    "print(\"After sanitizing\", df_san['sanitized'].apply(lambda x: len(x.split(' '))).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0450d0",
   "metadata": {},
   "source": [
    "Just to check, we see that before sanitizing, we had 18'398'298 words. We were able to halve it to 9'575'942 by sanitization and stemming our tweets.\n",
    "\n",
    "Before fitting our models, we also take a subsample to be able to compute them faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "43d596ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Id 423388\n",
      "Last Id 63949\n",
      "Length 128000\n"
     ]
    }
   ],
   "source": [
    "df_sub = df_san.sample(frac=0.1, random_state=SEED)\n",
    "\n",
    "# To check reproducibility\n",
    "print(\"First Id\", df_sub[\"Id\"].iloc[0])\n",
    "print(\"Last Id\", df_sub[\"Id\"].iloc[-1])\n",
    "print(\"Length\", df_sub.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf3e2ef",
   "metadata": {},
   "source": [
    "# 2. Fitting\n",
    "\n",
    "Now that we have a subsample (10%) of our cleaned data, we can try to fit some models to see what it gives us. We also define a standard 10 kfolds which we will use for our cross-validation. \n",
    "\n",
    "Just as a note to justify the 10% choice for our sample. The bernoulli classifier below achieves, with default settings, an accuracy score of `76.37%` when using the whole dataset. For 10% of the dataset, the accuracy drops to `75.24%`. A 1% drop in accuracy for 10 times less computation time seems worth it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81587b37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "30840893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_sub[\"sanitized\"], df_sub[\"emotion\"], \n",
    "                                                    test_size=0.2, shuffle=True, random_state=SEED)\n",
    "\n",
    "#only 4 folds because I have 4 cores, just to test\n",
    "folds = KFold(n_splits=4, shuffle=True, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "770d097f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  (102400,)\n",
      "X_test:  (25600,)\n",
      "y_train:  (102400,)\n",
      "y_test:  (25600,)\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "print(\"X_train: \", X_train.shape)\n",
    "print(\"X_test: \", X_test.shape)\n",
    "print(\"y_train: \", y_train.shape)\n",
    "print(\"y_test: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b670195",
   "metadata": {},
   "source": [
    "First model we will try is the `BernoulliNB` since we have binary data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7f67e4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 2.146036386489868\n",
      "Mean CV accuracy: 0.7483203125\n",
      "Test accuracy: 0.7523828125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nWhole dataset:\\nTime 21.568854093551636\\nMean CV accuracy: 0.7639306640625\\nTest accuracy: 0.76375390625\\n'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "berNB = Pipeline(\n",
    "    [\n",
    "        (\"tfidf\", TfidfVectorizer()),\n",
    "        (\"clf\", BernoulliNB()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "CV_ber = cross_val_score(\n",
    "    berNB, X_train, y_train, scoring=\"accuracy\", cv=folds, n_jobs=-1\n",
    ")\n",
    "\n",
    "berNB.fit(X_train, y_train)\n",
    "y_pred = berNB.predict(X_test)\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "print(f\"Time {time.time() - start}\")\n",
    "print(f\"Mean CV accuracy: {np.mean(CV_ber)}\")\n",
    "print(f\"Test accuracy: {score}\")\n",
    "\n",
    "\"\"\"\n",
    "Whole dataset:\n",
    "Time 21.568854093551636\n",
    "Mean CV accuracy: 0.7639306640625\n",
    "Test accuracy: 0.76375390625\n",
    "\n",
    "10% sample:\n",
    "Time 2.146036386489868\n",
    "Mean CV accuracy: 0.7483203125\n",
    "Test accuracy: 0.7523828125\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970cd7e2",
   "metadata": {},
   "source": [
    "Without any paramter tuning, we got 75% with the naive Bayes Bernoulli classifier.\n",
    "Baseline is `0.77309`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a8b06038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 23.104999542236328\n",
      "Test accuracy: 0.7562109375\n"
     ]
    }
   ],
   "source": [
    "berNB = Pipeline(\n",
    "    [\n",
    "        (\"tfidf\", TfidfVectorizer()),\n",
    "        (\"clf\", BernoulliNB()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "gridsearch  = GridSearchCV(\n",
    "    berNB,\n",
    "    {\n",
    "        \"clf__alpha\": np.linspace(5, 8, 10),\n",
    "        \"clf__fit_prior\": [False, True]\n",
    "    },\n",
    "    scoring=\"accuracy\",\n",
    "    cv=folds,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "gridsearch.fit(X_train, y_train)\n",
    "y_pred = gridsearch.predict(X_test)\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "\n",
    "res = gridsearch.cv_results_\n",
    "resdf = pd.DataFrame(res)\n",
    "print(f\"Time {time.time() - start}\")\n",
    "print(f\"Test accuracy: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e286ed8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_clf__alpha</th>\n",
       "      <th>param_clf__fit_prior</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.735499</td>\n",
       "      <td>0.028929</td>\n",
       "      <td>0.223002</td>\n",
       "      <td>0.016716</td>\n",
       "      <td>6.0</td>\n",
       "      <td>False</td>\n",
       "      <td>{'clf__alpha': 6.0, 'clf__fit_prior': False}</td>\n",
       "      <td>0.749531</td>\n",
       "      <td>0.753945</td>\n",
       "      <td>0.753477</td>\n",
       "      <td>0.757656</td>\n",
       "      <td>0.753652</td>\n",
       "      <td>0.002878</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.884501</td>\n",
       "      <td>0.070436</td>\n",
       "      <td>0.406499</td>\n",
       "      <td>0.120748</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>False</td>\n",
       "      <td>{'clf__alpha': 6.333333333333333, 'clf__fit_pr...</td>\n",
       "      <td>0.749609</td>\n",
       "      <td>0.753906</td>\n",
       "      <td>0.753516</td>\n",
       "      <td>0.757383</td>\n",
       "      <td>0.753604</td>\n",
       "      <td>0.002754</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.095748</td>\n",
       "      <td>0.162309</td>\n",
       "      <td>0.226250</td>\n",
       "      <td>0.014618</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>True</td>\n",
       "      <td>{'clf__alpha': 6.333333333333333, 'clf__fit_pr...</td>\n",
       "      <td>0.749687</td>\n",
       "      <td>0.753945</td>\n",
       "      <td>0.753242</td>\n",
       "      <td>0.757383</td>\n",
       "      <td>0.753564</td>\n",
       "      <td>0.002732</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.722748</td>\n",
       "      <td>0.011077</td>\n",
       "      <td>0.218499</td>\n",
       "      <td>0.021843</td>\n",
       "      <td>7.0</td>\n",
       "      <td>False</td>\n",
       "      <td>{'clf__alpha': 7.0, 'clf__fit_prior': False}</td>\n",
       "      <td>0.749922</td>\n",
       "      <td>0.754297</td>\n",
       "      <td>0.753125</td>\n",
       "      <td>0.756836</td>\n",
       "      <td>0.753545</td>\n",
       "      <td>0.002485</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.806748</td>\n",
       "      <td>0.025221</td>\n",
       "      <td>0.229752</td>\n",
       "      <td>0.030588</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>False</td>\n",
       "      <td>{'clf__alpha': 6.666666666666666, 'clf__fit_pr...</td>\n",
       "      <td>0.749805</td>\n",
       "      <td>0.754102</td>\n",
       "      <td>0.753320</td>\n",
       "      <td>0.756914</td>\n",
       "      <td>0.753535</td>\n",
       "      <td>0.002535</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.895500</td>\n",
       "      <td>0.028935</td>\n",
       "      <td>0.323000</td>\n",
       "      <td>0.043030</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>False</td>\n",
       "      <td>{'clf__alpha': 5.666666666666667, 'clf__fit_pr...</td>\n",
       "      <td>0.749687</td>\n",
       "      <td>0.753672</td>\n",
       "      <td>0.753398</td>\n",
       "      <td>0.757227</td>\n",
       "      <td>0.753496</td>\n",
       "      <td>0.002667</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.758748</td>\n",
       "      <td>0.019107</td>\n",
       "      <td>0.205500</td>\n",
       "      <td>0.009340</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>True</td>\n",
       "      <td>{'clf__alpha': 6.666666666666666, 'clf__fit_pr...</td>\n",
       "      <td>0.749805</td>\n",
       "      <td>0.753906</td>\n",
       "      <td>0.753359</td>\n",
       "      <td>0.756836</td>\n",
       "      <td>0.753477</td>\n",
       "      <td>0.002498</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.703750</td>\n",
       "      <td>0.012637</td>\n",
       "      <td>0.202998</td>\n",
       "      <td>0.014850</td>\n",
       "      <td>6.0</td>\n",
       "      <td>True</td>\n",
       "      <td>{'clf__alpha': 6.0, 'clf__fit_prior': True}</td>\n",
       "      <td>0.749609</td>\n",
       "      <td>0.753437</td>\n",
       "      <td>0.753047</td>\n",
       "      <td>0.757734</td>\n",
       "      <td>0.753457</td>\n",
       "      <td>0.002884</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.742747</td>\n",
       "      <td>0.022719</td>\n",
       "      <td>0.198001</td>\n",
       "      <td>0.006364</td>\n",
       "      <td>7.0</td>\n",
       "      <td>True</td>\n",
       "      <td>{'clf__alpha': 7.0, 'clf__fit_prior': True}</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.753906</td>\n",
       "      <td>0.753281</td>\n",
       "      <td>0.756641</td>\n",
       "      <td>0.753457</td>\n",
       "      <td>0.002362</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.763747</td>\n",
       "      <td>0.021287</td>\n",
       "      <td>0.215250</td>\n",
       "      <td>0.015990</td>\n",
       "      <td>7.333333</td>\n",
       "      <td>False</td>\n",
       "      <td>{'clf__alpha': 7.333333333333333, 'clf__fit_pr...</td>\n",
       "      <td>0.749805</td>\n",
       "      <td>0.753945</td>\n",
       "      <td>0.753281</td>\n",
       "      <td>0.756719</td>\n",
       "      <td>0.753437</td>\n",
       "      <td>0.002462</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.781249</td>\n",
       "      <td>0.044551</td>\n",
       "      <td>0.217249</td>\n",
       "      <td>0.014840</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>True</td>\n",
       "      <td>{'clf__alpha': 5.666666666666667, 'clf__fit_pr...</td>\n",
       "      <td>0.749727</td>\n",
       "      <td>0.753477</td>\n",
       "      <td>0.752891</td>\n",
       "      <td>0.757500</td>\n",
       "      <td>0.753398</td>\n",
       "      <td>0.002764</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.711248</td>\n",
       "      <td>0.015121</td>\n",
       "      <td>0.204250</td>\n",
       "      <td>0.014938</td>\n",
       "      <td>7.666667</td>\n",
       "      <td>False</td>\n",
       "      <td>{'clf__alpha': 7.666666666666666, 'clf__fit_pr...</td>\n",
       "      <td>0.749414</td>\n",
       "      <td>0.753828</td>\n",
       "      <td>0.753281</td>\n",
       "      <td>0.756914</td>\n",
       "      <td>0.753359</td>\n",
       "      <td>0.002666</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.716498</td>\n",
       "      <td>0.031792</td>\n",
       "      <td>0.199750</td>\n",
       "      <td>0.011860</td>\n",
       "      <td>7.333333</td>\n",
       "      <td>True</td>\n",
       "      <td>{'clf__alpha': 7.333333333333333, 'clf__fit_pr...</td>\n",
       "      <td>0.749844</td>\n",
       "      <td>0.753906</td>\n",
       "      <td>0.752773</td>\n",
       "      <td>0.756445</td>\n",
       "      <td>0.753242</td>\n",
       "      <td>0.002370</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.887498</td>\n",
       "      <td>0.014842</td>\n",
       "      <td>0.265000</td>\n",
       "      <td>0.046158</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>True</td>\n",
       "      <td>{'clf__alpha': 5.333333333333333, 'clf__fit_pr...</td>\n",
       "      <td>0.749492</td>\n",
       "      <td>0.753594</td>\n",
       "      <td>0.752539</td>\n",
       "      <td>0.757148</td>\n",
       "      <td>0.753193</td>\n",
       "      <td>0.002735</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.705498</td>\n",
       "      <td>0.026780</td>\n",
       "      <td>0.197501</td>\n",
       "      <td>0.011055</td>\n",
       "      <td>7.666667</td>\n",
       "      <td>True</td>\n",
       "      <td>{'clf__alpha': 7.666666666666666, 'clf__fit_pr...</td>\n",
       "      <td>0.749453</td>\n",
       "      <td>0.753828</td>\n",
       "      <td>0.752695</td>\n",
       "      <td>0.756797</td>\n",
       "      <td>0.753193</td>\n",
       "      <td>0.002628</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.726249</td>\n",
       "      <td>0.012072</td>\n",
       "      <td>0.206000</td>\n",
       "      <td>0.019518</td>\n",
       "      <td>8.0</td>\n",
       "      <td>False</td>\n",
       "      <td>{'clf__alpha': 8.0, 'clf__fit_prior': False}</td>\n",
       "      <td>0.749141</td>\n",
       "      <td>0.753242</td>\n",
       "      <td>0.753086</td>\n",
       "      <td>0.757070</td>\n",
       "      <td>0.753135</td>\n",
       "      <td>0.002804</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.864251</td>\n",
       "      <td>0.031285</td>\n",
       "      <td>0.247999</td>\n",
       "      <td>0.019105</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>False</td>\n",
       "      <td>{'clf__alpha': 5.333333333333333, 'clf__fit_pr...</td>\n",
       "      <td>0.749492</td>\n",
       "      <td>0.753320</td>\n",
       "      <td>0.752852</td>\n",
       "      <td>0.756875</td>\n",
       "      <td>0.753135</td>\n",
       "      <td>0.002616</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.665246</td>\n",
       "      <td>0.011755</td>\n",
       "      <td>0.184752</td>\n",
       "      <td>0.002488</td>\n",
       "      <td>8.0</td>\n",
       "      <td>True</td>\n",
       "      <td>{'clf__alpha': 8.0, 'clf__fit_prior': True}</td>\n",
       "      <td>0.749219</td>\n",
       "      <td>0.753594</td>\n",
       "      <td>0.752578</td>\n",
       "      <td>0.756758</td>\n",
       "      <td>0.753037</td>\n",
       "      <td>0.002690</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.685492</td>\n",
       "      <td>0.012060</td>\n",
       "      <td>0.255000</td>\n",
       "      <td>0.027652</td>\n",
       "      <td>5.0</td>\n",
       "      <td>False</td>\n",
       "      <td>{'clf__alpha': 5.0, 'clf__fit_prior': False}</td>\n",
       "      <td>0.749531</td>\n",
       "      <td>0.753281</td>\n",
       "      <td>0.752500</td>\n",
       "      <td>0.756719</td>\n",
       "      <td>0.753008</td>\n",
       "      <td>0.002559</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.686998</td>\n",
       "      <td>0.019964</td>\n",
       "      <td>0.221749</td>\n",
       "      <td>0.021076</td>\n",
       "      <td>5.0</td>\n",
       "      <td>True</td>\n",
       "      <td>{'clf__alpha': 5.0, 'clf__fit_prior': True}</td>\n",
       "      <td>0.749648</td>\n",
       "      <td>0.753281</td>\n",
       "      <td>0.752383</td>\n",
       "      <td>0.756445</td>\n",
       "      <td>0.752939</td>\n",
       "      <td>0.002426</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "6        0.735499      0.028929         0.223002        0.016716   \n",
       "8        0.884501      0.070436         0.406499        0.120748   \n",
       "9        1.095748      0.162309         0.226250        0.014618   \n",
       "12       0.722748      0.011077         0.218499        0.021843   \n",
       "10       0.806748      0.025221         0.229752        0.030588   \n",
       "4        0.895500      0.028935         0.323000        0.043030   \n",
       "11       0.758748      0.019107         0.205500        0.009340   \n",
       "7        0.703750      0.012637         0.202998        0.014850   \n",
       "13       0.742747      0.022719         0.198001        0.006364   \n",
       "14       0.763747      0.021287         0.215250        0.015990   \n",
       "5        0.781249      0.044551         0.217249        0.014840   \n",
       "16       0.711248      0.015121         0.204250        0.014938   \n",
       "15       0.716498      0.031792         0.199750        0.011860   \n",
       "3        0.887498      0.014842         0.265000        0.046158   \n",
       "17       0.705498      0.026780         0.197501        0.011055   \n",
       "18       0.726249      0.012072         0.206000        0.019518   \n",
       "2        0.864251      0.031285         0.247999        0.019105   \n",
       "19       0.665246      0.011755         0.184752        0.002488   \n",
       "0        0.685492      0.012060         0.255000        0.027652   \n",
       "1        0.686998      0.019964         0.221749        0.021076   \n",
       "\n",
       "   param_clf__alpha param_clf__fit_prior  \\\n",
       "6               6.0                False   \n",
       "8          6.333333                False   \n",
       "9          6.333333                 True   \n",
       "12              7.0                False   \n",
       "10         6.666667                False   \n",
       "4          5.666667                False   \n",
       "11         6.666667                 True   \n",
       "7               6.0                 True   \n",
       "13              7.0                 True   \n",
       "14         7.333333                False   \n",
       "5          5.666667                 True   \n",
       "16         7.666667                False   \n",
       "15         7.333333                 True   \n",
       "3          5.333333                 True   \n",
       "17         7.666667                 True   \n",
       "18              8.0                False   \n",
       "2          5.333333                False   \n",
       "19              8.0                 True   \n",
       "0               5.0                False   \n",
       "1               5.0                 True   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "6        {'clf__alpha': 6.0, 'clf__fit_prior': False}           0.749531   \n",
       "8   {'clf__alpha': 6.333333333333333, 'clf__fit_pr...           0.749609   \n",
       "9   {'clf__alpha': 6.333333333333333, 'clf__fit_pr...           0.749687   \n",
       "12       {'clf__alpha': 7.0, 'clf__fit_prior': False}           0.749922   \n",
       "10  {'clf__alpha': 6.666666666666666, 'clf__fit_pr...           0.749805   \n",
       "4   {'clf__alpha': 5.666666666666667, 'clf__fit_pr...           0.749687   \n",
       "11  {'clf__alpha': 6.666666666666666, 'clf__fit_pr...           0.749805   \n",
       "7         {'clf__alpha': 6.0, 'clf__fit_prior': True}           0.749609   \n",
       "13        {'clf__alpha': 7.0, 'clf__fit_prior': True}           0.750000   \n",
       "14  {'clf__alpha': 7.333333333333333, 'clf__fit_pr...           0.749805   \n",
       "5   {'clf__alpha': 5.666666666666667, 'clf__fit_pr...           0.749727   \n",
       "16  {'clf__alpha': 7.666666666666666, 'clf__fit_pr...           0.749414   \n",
       "15  {'clf__alpha': 7.333333333333333, 'clf__fit_pr...           0.749844   \n",
       "3   {'clf__alpha': 5.333333333333333, 'clf__fit_pr...           0.749492   \n",
       "17  {'clf__alpha': 7.666666666666666, 'clf__fit_pr...           0.749453   \n",
       "18       {'clf__alpha': 8.0, 'clf__fit_prior': False}           0.749141   \n",
       "2   {'clf__alpha': 5.333333333333333, 'clf__fit_pr...           0.749492   \n",
       "19        {'clf__alpha': 8.0, 'clf__fit_prior': True}           0.749219   \n",
       "0        {'clf__alpha': 5.0, 'clf__fit_prior': False}           0.749531   \n",
       "1         {'clf__alpha': 5.0, 'clf__fit_prior': True}           0.749648   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  mean_test_score  \\\n",
       "6            0.753945           0.753477           0.757656         0.753652   \n",
       "8            0.753906           0.753516           0.757383         0.753604   \n",
       "9            0.753945           0.753242           0.757383         0.753564   \n",
       "12           0.754297           0.753125           0.756836         0.753545   \n",
       "10           0.754102           0.753320           0.756914         0.753535   \n",
       "4            0.753672           0.753398           0.757227         0.753496   \n",
       "11           0.753906           0.753359           0.756836         0.753477   \n",
       "7            0.753437           0.753047           0.757734         0.753457   \n",
       "13           0.753906           0.753281           0.756641         0.753457   \n",
       "14           0.753945           0.753281           0.756719         0.753437   \n",
       "5            0.753477           0.752891           0.757500         0.753398   \n",
       "16           0.753828           0.753281           0.756914         0.753359   \n",
       "15           0.753906           0.752773           0.756445         0.753242   \n",
       "3            0.753594           0.752539           0.757148         0.753193   \n",
       "17           0.753828           0.752695           0.756797         0.753193   \n",
       "18           0.753242           0.753086           0.757070         0.753135   \n",
       "2            0.753320           0.752852           0.756875         0.753135   \n",
       "19           0.753594           0.752578           0.756758         0.753037   \n",
       "0            0.753281           0.752500           0.756719         0.753008   \n",
       "1            0.753281           0.752383           0.756445         0.752939   \n",
       "\n",
       "    std_test_score  rank_test_score  \n",
       "6         0.002878                1  \n",
       "8         0.002754                2  \n",
       "9         0.002732                3  \n",
       "12        0.002485                4  \n",
       "10        0.002535                5  \n",
       "4         0.002667                6  \n",
       "11        0.002498                7  \n",
       "7         0.002884                8  \n",
       "13        0.002362                8  \n",
       "14        0.002462               10  \n",
       "5         0.002764               11  \n",
       "16        0.002666               12  \n",
       "15        0.002370               13  \n",
       "3         0.002735               14  \n",
       "17        0.002628               14  \n",
       "18        0.002804               16  \n",
       "2         0.002616               16  \n",
       "19        0.002690               18  \n",
       "0         0.002559               19  \n",
       "1         0.002426               20  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resdf.sort_values(by=[\"rank_test_score\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65661d1",
   "metadata": {},
   "source": [
    "No matter the alpha, we don't seem to get higher than 0.75. BernoulliNB is not the best model here.\n",
    "Instead of having to write the same for every model, let's try to automatize the testing just by specifying the model and the parameters grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "3a953e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models = [LogisticRegression(warm_start=True)]\n",
    "models = [SGDClassifier(warm_start=True)]\n",
    "models = [BaggingClassifier(warm_start=True)]\n",
    "models = [RandomForestClassifier(warm_start=True)]\n",
    "models = [BaggingClassifier(warm_start=True)]\n",
    "models = [SVC()]\n",
    "models = [LinearSVC()]\n",
    "models = [RidgeClassifier()]\n",
    "models = [BernoulliNB()]\n",
    "\n",
    "params_tfid = {\n",
    "    \"tfidfvectorizer__norm\": [\"l2\"],\n",
    "    \"tfidfvectorizer__analyzer\": [\"word\"],\n",
    "    #\"tfidfvectorizer__ngram_range\": [(2,3), (2,2)],\n",
    "    #\"tfidfvectorizer__use_idf\": [True, False],\n",
    "    #\"tfidfvectorizer__smooth_idf\": [True, False],\n",
    "    #\"tfidfvectorizer__sublinear_tf\": [True, False]\n",
    "}\n",
    "\n",
    "params = {\n",
    "    \"bernoullinb\": {\n",
    "        \"bernoullinb__alpha\": [7.5],\n",
    "        \"bernoullinb__fit_prior\": [False, True],\n",
    "    },\n",
    "    \"ridgeclassifier\": {\n",
    "        \"ridgeclassifier__alpha\": np.linspace(1e-5, 10, 5),\n",
    "        \"ridgeclassifier__class_weight\": [\"balanced\", None],\n",
    "        \"ridgeclassifier__normalize\": [False, True],\n",
    "        \n",
    "    },\n",
    "    \"logisticregression\": {\n",
    "        #\"logisticregression__penalty\": [\"l1\", \"l2\", \"elasticnet\"],\n",
    "        \"logisticregression__penalty\": [\"l1\"],\n",
    "        #\"logisticregression__dual\": [False, True], try with liblinear\n",
    "        \"logisticregression__C\": 10**np.linspace(-3, -0.001, 4),\n",
    "        #\"logisticregression__C\": [1e-1],\n",
    "        \"logisticregression__random_state\": [SEED],\n",
    "        #\"logisticregression__solver\": [\"newton-cg\", \"lbfgs\", \"saga\"],\n",
    "        \"logisticregression__solver\": [\"saga\"],\n",
    "        #\"logisticregression__l1_ratio\": np.linspace(0.1, 0.9, 5),\n",
    "    },\n",
    "    \"sgdclassifier\": {\n",
    "        \"sgdclassifier__random_state\": [SEED],\n",
    "        \"sgdclassifier__loss\": [\"modified_huber\"],\n",
    "        \"sgdclassifier__alpha\": 10**np.linspace(-3, -0.001, 4),\n",
    "    },\n",
    "    \"baggingclassifier\": {\n",
    "        \"baggingclassifier__random_state\": [SEED],\n",
    "        \"baggingclassifier__n_estimators\": [30],\n",
    "        \"baggingclassifier__max_samples\": [0.05],\n",
    "        \"baggingclassifier__max_features\": [0.5],\n",
    "        \n",
    "    },\n",
    "    \"randomforestclassifier\": {\n",
    "        \"randomforestclassifier__random_state\": [SEED],\n",
    "    },\n",
    "    \"svc\": {\n",
    "        \"svc__random_state\": [SEED],\n",
    "    },\n",
    "    \"linearsvc\": {\n",
    "        \"linearsvc__random_state\": [SEED],\n",
    "        \"linearsvc__loss\": [\"squared_hinge\"],\n",
    "        \"linearsvc__penalty\": [\"l2\"],\n",
    "        \"linearsvc__max_iter\": [1000],\n",
    "        \"linearsvc__dual\": [False],\n",
    "        \"linearsvc__C\": np.linspace(0.01, 0.2, 10),\n",
    "        \"linearsvc__class_weight\": [\"balanced\"],\n",
    "    }\n",
    "}\n",
    "\n",
    "# If we also want to gridsearch the different Tfidf params\n",
    "for k, v in params_tfid.items():\n",
    "    params[\"bernoullinb\"][k] = v\n",
    "    #Easier if we comment above\n",
    "    pass\n",
    "\n",
    "pipes = []\n",
    "\n",
    "# Also check what we can do with the TfidfVectorizer parameters\n",
    "for model in models:\n",
    "    pipe = make_pipeline(TfidfVectorizer(), model)\n",
    "    pipes.append(pipe)\n",
    "    \n",
    "    # Will use that once we have the best params\n",
    "    #pipe.set_params(**params[pipe.steps[1][0]])\n",
    "\n",
    "# Initialize empty dictionary\n",
    "reports = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "24921704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00000000e-05, 3.68600191e-05, 1.35866101e-04, 5.00802706e-04,\n",
       "       1.84595973e-03, 6.80421108e-03, 2.50803350e-02, 9.24461627e-02,\n",
       "       3.40756732e-01, 1.25602996e+00])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10**np.linspace(-5, 0.099, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b65d9d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bernoullinb\n",
      "Fitting 4 folds for each of 2 candidates, totalling 8 fits\n",
      "Time 3.202148675918579s\n",
      "Test accuracy: 0.7553125\n"
     ]
    }
   ],
   "source": [
    "# Fit each different pipeline\n",
    "\n",
    "for pipe in pipes:\n",
    "    print(pipe.steps[1][0])\n",
    "    start = time.time()\n",
    "    \n",
    "    gridsearch = GridSearchCV(pipe, params[pipe.steps[1][0]], scoring=\"accuracy\", cv=folds, n_jobs=-1, verbose=3)\n",
    "    gridsearch.fit(X_train, y_train)\n",
    "    y_pred = gridsearch.predict(X_test)\n",
    "    \n",
    "    score = accuracy_score(y_test, y_pred)\n",
    "    resdf = pd.DataFrame(gridsearch.cv_results_)\n",
    "    \n",
    "    reports[pipe.steps[1][0]] = classification_report(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Time {time.time() - start}s\")\n",
    "    #print(resdf[resdf[\"rank_test_score\"] == 1])\n",
    "    print(f\"Test accuracy: {score}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "c8cd5ae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_bernoullinb__alpha</th>\n",
       "      <th>param_bernoullinb__fit_prior</th>\n",
       "      <th>param_tfidfvectorizer__analyzer</th>\n",
       "      <th>param_tfidfvectorizer__norm</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.686249</td>\n",
       "      <td>0.014220</td>\n",
       "      <td>0.224001</td>\n",
       "      <td>0.020384</td>\n",
       "      <td>7.5</td>\n",
       "      <td>False</td>\n",
       "      <td>word</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'bernoullinb__alpha': 7.5, 'bernoullinb__fit_...</td>\n",
       "      <td>0.749570</td>\n",
       "      <td>0.753828</td>\n",
       "      <td>0.753437</td>\n",
       "      <td>0.756953</td>\n",
       "      <td>0.753447</td>\n",
       "      <td>0.00262</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.663750</td>\n",
       "      <td>0.016436</td>\n",
       "      <td>0.193498</td>\n",
       "      <td>0.006983</td>\n",
       "      <td>7.5</td>\n",
       "      <td>True</td>\n",
       "      <td>word</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'bernoullinb__alpha': 7.5, 'bernoullinb__fit_...</td>\n",
       "      <td>0.749844</td>\n",
       "      <td>0.754023</td>\n",
       "      <td>0.752734</td>\n",
       "      <td>0.756563</td>\n",
       "      <td>0.753291</td>\n",
       "      <td>0.00242</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       0.686249      0.014220         0.224001        0.020384   \n",
       "1       0.663750      0.016436         0.193498        0.006983   \n",
       "\n",
       "  param_bernoullinb__alpha param_bernoullinb__fit_prior  \\\n",
       "0                      7.5                        False   \n",
       "1                      7.5                         True   \n",
       "\n",
       "  param_tfidfvectorizer__analyzer param_tfidfvectorizer__norm  \\\n",
       "0                            word                          l2   \n",
       "1                            word                          l2   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "0  {'bernoullinb__alpha': 7.5, 'bernoullinb__fit_...           0.749570   \n",
       "1  {'bernoullinb__alpha': 7.5, 'bernoullinb__fit_...           0.749844   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  mean_test_score  \\\n",
       "0           0.753828           0.753437           0.756953         0.753447   \n",
       "1           0.754023           0.752734           0.756563         0.753291   \n",
       "\n",
       "   std_test_score  rank_test_score  \n",
       "0         0.00262                1  \n",
       "1         0.00242                2  "
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resdf.sort_values(by=[\"rank_test_score\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a040074c",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "Linear models: <br>\n",
    "BernoulliNB: 0.76 <br>\n",
    "LogisticRegression: <br>\n",
    "    l2 - 0.76 <br>\n",
    "    l2, C range - 0.76 <br>\n",
    "    elasticnet, C range, l1_ratio range - 0.75 <br>\n",
    "    l1, C=0.1 - 0.74 <br>\n",
    "    l1, C range - 0.76 <br>\n",
    "sgdclassifier: <br>\n",
    "    all default - 0.75 <br>\n",
    "    modified_huber - 0.75 <br>\n",
    "ridgeclassifier: <br>\n",
    "    alpha=7.5, class_weight=balanced, normalize=False -> 0.76\n",
    "    \n",
    "Linear models seem not to perform better than around 0.75. Let's go to ensembles.\n",
    "\n",
    "Ensemble: <br>\n",
    "RandomForestClassifier: <br>\n",
    "    max_samples 0.001 - 0.59 <br>\n",
    "    max_samples 0.1 - 0.71 <br>\n",
    "    max_samples 0.8 - 0.73 <br>\n",
    "    max_samples 0.01, n_estimators 40 - 0.71 <br>\n",
    "    max_samples 0.01, n_estimators 20 - 0.68 <br>\n",
    "    max_samples 0.01, n_estimators 100 - 0.72 <br>\n",
    "    max_samples 0.1, n_estimators 50 - 0.74 <br>\n",
    "    max_samples 0.5, n_estimators 70 - 0.75 <br>\n",
    "    max_samples 0.1, n_estimators 150 - 0.75 <br>\n",
    "\n",
    "BaggingClassifier: <br>\n",
    "    max_samples 0.01, n_estimators 100 - 0.70 <br>\n",
    "    max_samples 0.1, n_estimators 10 - 0.72 <br>\n",
    "    max_samples 0.05, n_estimators 30 - 0.72 <br>\n",
    "    max_samples 0.05, n_estimators 30, max_features 0.5 - 0.72 <br>\n",
    "    \n",
    "SVC: -> not fast enough\n",
    " \n",
    "LinearSVC: <br>\n",
    "default: 0.75 -> really fast! <br>\n",
    "squared_hinge + dual=False + C=0.1 -> 0.76 <br>\n",
    "squared_hinge + dual=False + C=0.1 + class_weight=balanced -> 0.76\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b43e695",
   "metadata": {},
   "source": [
    "No matter the classifier, we don't seem to go above 76%. Look into the data to see if we can see something.\n",
    "-> delete very short tweets?\n",
    "-> lemmatization?\n",
    "\n",
    "Maybe deleted too much noise during preprocessing, only remove stopwords? (+URLs?)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
