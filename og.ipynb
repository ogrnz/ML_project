{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b02a622",
   "metadata": {},
   "source": [
    "## ML competition\n",
    "\n",
    "_Marilyn, Shiva, Olivier_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98d437c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup chunk\n",
    "import time\n",
    "\n",
    "# Data wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Text sanitization\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "stemmer = SnowballStemmer(language=\"english\")\n",
    "\n",
    "# Lang detection\n",
    "import langid\n",
    "from langid.langid import LanguageIdentifier, model\n",
    "identifier = LanguageIdentifier.from_modelstring(model, norm_probs=True)\n",
    "\n",
    "# Misc\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# Define the seed for reproducibility\n",
    "SEED = 31415"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2702350e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit time\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import (\n",
    "    LinearDiscriminantAnalysis,\n",
    "    QuadraticDiscriminantAnalysis,\n",
    ")\n",
    "\n",
    "from sklearn.feature_extraction.text import (\n",
    "    CountVectorizer, \n",
    "    TfidfTransformer, \n",
    "    TfidfVectorizer\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, \n",
    "    GridSearchCV, \n",
    "    KFold, \n",
    "    cross_val_score\n",
    ")\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    accuracy_score, \n",
    "    confusion_matrix\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4e33df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/MLUnige2021_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fb1699",
   "metadata": {},
   "source": [
    "### 1. EDA\n",
    "Small EDA to check a bit the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "863f3ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1280000 entries, 0 to 1279999\n",
      "Data columns (total 7 columns):\n",
      " #   Column     Non-Null Count    Dtype \n",
      "---  ------     --------------    ----- \n",
      " 0   Id         1280000 non-null  int64 \n",
      " 1   emotion    1280000 non-null  int64 \n",
      " 2   tweet_id   1280000 non-null  int64 \n",
      " 3   date       1280000 non-null  object\n",
      " 4   lyx_query  1280000 non-null  object\n",
      " 5   user       1280000 non-null  object\n",
      " 6   text       1280000 non-null  object\n",
      "dtypes: int64(3), object(4)\n",
      "memory usage: 68.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08511c3f",
   "metadata": {},
   "source": [
    "More than a million entries. What is `lyx_query`? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8057a64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NO_QUERY'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"lyx_query\"].head()\n",
    "df[\"lyx_query\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323ea04f",
   "metadata": {},
   "source": [
    "Welp only `\"NO_QUERY\"` so we can drop it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38f60e34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          2063391019\n",
       "1          2000525676\n",
       "2          2218180611\n",
       "3          2190269101\n",
       "4          2069249490\n",
       "              ...    \n",
       "1279995    1835296397\n",
       "1279996    2226720395\n",
       "1279997    1962176213\n",
       "1279998    1976894947\n",
       "1279999    1563596981\n",
       "Name: tweet_id, Length: 1280000, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"tweet_id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8d30ed",
   "metadata": {},
   "source": [
    "Those are old tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e4dd307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          Sun Jun 07 02:28:13 PDT 2009\n",
       "1          Mon Jun 01 22:18:53 PDT 2009\n",
       "2          Wed Jun 17 22:01:38 PDT 2009\n",
       "3          Tue Jun 16 02:14:47 PDT 2009\n",
       "4          Sun Jun 07 15:31:58 PDT 2009\n",
       "                       ...             \n",
       "1279995    Mon May 18 05:39:18 PDT 2009\n",
       "1279996    Thu Jun 18 12:18:05 PDT 2009\n",
       "1279997    Fri May 29 10:38:30 PDT 2009\n",
       "1279998    Sat May 30 19:28:13 PDT 2009\n",
       "1279999    Sun Apr 19 23:27:25 PDT 2009\n",
       "Name: date, Length: 1280000, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"date\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a9a7fc",
   "metadata": {},
   "source": [
    "Indeed they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec133b90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "574114"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[\"user\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422dad4b",
   "metadata": {},
   "source": [
    "Lots of different users. If we had only like 1000s of users, we could have looked for some pattern (user X is only positive ...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06fa4373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emotion                                                    1\n",
       "text       @BreeMe more time to play with you BlackBerry ...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0, [\"emotion\", \"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "125d6bc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"emotion\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac0c8477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "236"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"emotion\"]==1].shape[0] - df[df[\"emotion\"]==0].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37efd97e",
   "metadata": {},
   "source": [
    "Perfectly balanced dataset (236 diff between the 2 classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6773c7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.bar([\"negative\", \"positive\"], [df[df[\"emotion\"]==0].shape[0], df[df[\"emotion\"]==1].shape[0]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7601343",
   "metadata": {},
   "source": [
    "Let's also check the language of the tweets (all eng or also others?). For that, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "500ab5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lang_detect(txt, threshold=0.95):\n",
    "    \"\"\"\n",
    "    Detect tweet language\n",
    "    returns None if confidence lvl < threshold\n",
    "    \"\"\"\n",
    "\n",
    "    if txt is None:\n",
    "        return None\n",
    "\n",
    "    txt = txt.replace(\"\\n\", \" \")\n",
    "    lang = identifier.classify(txt)\n",
    "    if lang[1] < threshold:\n",
    "        return None\n",
    "    else:\n",
    "        return lang[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "460fe624",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_samp = df.sample(150_000) #Total df takes 1h to run\n",
    "#df_samp[\"lang\"] = df_samp[\"text\"].progress_apply(lang_detect)\n",
    "#df_samp[\"lang\"].unique()\n",
    "#df_samp[~(df_samp[\"lang\"] == \"en\")].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bcc800",
   "metadata": {},
   "source": [
    "Only 15k tweets which are not detected as english in our 150k sample. By checking some of the tweets, most are english, but the language detector surely has some trouble with some very short tweets containing one or more foreign words.--> consider all as english!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a71c4f4",
   "metadata": {},
   "source": [
    "## 2. Strategy\n",
    "\n",
    "1. Preprocess the text\n",
    "    1. remove punctuation marks\n",
    "    2. remove stopwords (en)\n",
    "    3. stem or lemmatize the words\n",
    "    \n",
    "    \n",
    "2. take a sample of our whole dataset (200k?) to do our preliminary test. We can't do cross validation on the whole dataset.\n",
    "\n",
    "3. Begin to fit the models. \n",
    "    1. Pipeline with TfidfTransformer (or the other one I don't remember the name)\n",
    "    2. BerNB\n",
    "    3. LogisticRegression\n",
    "    4. RidgeClassifier\n",
    "    5. SGDClassifier\n",
    "    6. SVC\n",
    "    7. RandomForestClassifier\n",
    "    8. DecisionTreeClassifier\n",
    "    9. KNeighborsClassifier\n",
    "    10. LDA? QDA?\n",
    "4. Select a few of the best models, CV with bigger dataset\n",
    "5. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e987a3",
   "metadata": {},
   "source": [
    "## Misc\n",
    "\n",
    "- Should pay attention to some special chars which were not removed: \"I \\&lt;3 you\" (#2204), at first just remove it, but we could try investigate that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05da19fb",
   "metadata": {},
   "source": [
    "# 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "708c0f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 150000/150000 [00:17<00:00, 8613.29it/s]\n"
     ]
    }
   ],
   "source": [
    "def sanitize(text: str) -> str:\n",
    "    \"\"\"Sanitize a string.\"\"\"\n",
    "    \n",
    "    # Edited regex from @gruber\n",
    "    # https://gist.github.com/gruber/8891611\n",
    "    url_re = r\"\"\"(?i)\\b((?:https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\-]+[.](?:com|net|org|edu|gov)/)(?:[^\\s()<>{}\\[\\]]+|\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\))+(?:\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’])|(?:(?<!@)[a-z0-9]+(?:[.\\-][a-z0-9]+)*[.](?:com|net|org|edu|gov)\\b/?(?!@)))\"\"\"\n",
    "    \n",
    "    edited_text = re.sub(\n",
    "        \"  \", \" \", text\n",
    "    )  # replace double whitespace with single whitespace\n",
    "    edited_text = re.sub(\"@(?=.*\\w)[\\w]{1,15}\", \"\", edited_text) # remove twitter handle\n",
    "    edited_text = re.sub(\n",
    "        url_re, \"\", edited_text\n",
    "    )  # remove URL\n",
    "    edited_text = edited_text.split(\" \")  # split the sentence into array of strs\n",
    "    edited_text = \" \".join(\n",
    "        [char for char in edited_text if char != \"\"]\n",
    "    )  # remove any empty str from text\n",
    "    edited_text = edited_text.lower()  # lowercase\n",
    "    edited_text = re.sub(\"\\d+\", \"\", edited_text)  # Removing numerics\n",
    "    edited_text = re.split(\n",
    "        \"\\W+\", edited_text\n",
    "    )  # spliting based on whitespace or whitespaces\n",
    "    edited_text = \" \".join(\n",
    "        [stemmer.stem(word) for word in edited_text if word not in stopwords]\n",
    "    )  # Snowball Stemmer\n",
    "    \n",
    "    return edited_text\n",
    "\n",
    "# Some special strings to test \n",
    "txt1 = \"Hello @Shiva and @Marilyn!\"\n",
    "#print(\"Sanitized:\", sanitize(txt1))\n",
    "regex = \"@(?=.*\\w)[\\w]{1,15}\"\n",
    "#print(re.sub(regex, \"\", txt1))\n",
    "\n",
    "# As a test, sanitize the subsample\n",
    "df_samp[\"sanitized\"] = df_samp[\"text\"].progress_apply(sanitize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "09fc58ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>emotion</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>date</th>\n",
       "      <th>lyx_query</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>sanitized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>747649</th>\n",
       "      <td>747649</td>\n",
       "      <td>1</td>\n",
       "      <td>1970462059</td>\n",
       "      <td>Sat May 30 03:58:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AilidhDocherty</td>\n",
       "      <td>Chilling in the sun.. Although I feel my face ...</td>\n",
       "      <td>chill sun although feel face get burnt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525499</th>\n",
       "      <td>525499</td>\n",
       "      <td>0</td>\n",
       "      <td>1556216852</td>\n",
       "      <td>Sat Apr 18 22:02:04 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>shutupmeg</td>\n",
       "      <td>(@MatAshton) I'm having violent feelings towar...</td>\n",
       "      <td>violent feel toward bird outsid window shut w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>956297</th>\n",
       "      <td>956297</td>\n",
       "      <td>1</td>\n",
       "      <td>2057569623</td>\n",
       "      <td>Sat Jun 06 13:14:55 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tweeteradder10</td>\n",
       "      <td>@armybrat4life1 Get 100 followers a day using ...</td>\n",
       "      <td>get follow day use add everyon train pay vip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81013</th>\n",
       "      <td>81013</td>\n",
       "      <td>1</td>\n",
       "      <td>2056860638</td>\n",
       "      <td>Sat Jun 06 11:55:58 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Brennon_Pratt</td>\n",
       "      <td>Headed out to deposit some checks and stop by ...</td>\n",
       "      <td>head deposit check stop walmart get airsoft gu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408310</th>\n",
       "      <td>408310</td>\n",
       "      <td>1</td>\n",
       "      <td>1932816756</td>\n",
       "      <td>Tue May 26 22:17:16 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>eeelz</td>\n",
       "      <td>@suloshini Happy thoughts! Your mum is going t...</td>\n",
       "      <td>happi thought mum go fine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id  emotion    tweet_id                          date lyx_query  \\\n",
       "747649  747649        1  1970462059  Sat May 30 03:58:00 PDT 2009  NO_QUERY   \n",
       "525499  525499        0  1556216852  Sat Apr 18 22:02:04 PDT 2009  NO_QUERY   \n",
       "956297  956297        1  2057569623  Sat Jun 06 13:14:55 PDT 2009  NO_QUERY   \n",
       "81013    81013        1  2056860638  Sat Jun 06 11:55:58 PDT 2009  NO_QUERY   \n",
       "408310  408310        1  1932816756  Tue May 26 22:17:16 PDT 2009  NO_QUERY   \n",
       "\n",
       "                  user                                               text  \\\n",
       "747649  AilidhDocherty  Chilling in the sun.. Although I feel my face ...   \n",
       "525499       shutupmeg  (@MatAshton) I'm having violent feelings towar...   \n",
       "956297  tweeteradder10  @armybrat4life1 Get 100 followers a day using ...   \n",
       "81013    Brennon_Pratt  Headed out to deposit some checks and stop by ...   \n",
       "408310           eeelz  @suloshini Happy thoughts! Your mum is going t...   \n",
       "\n",
       "                                                sanitized  \n",
       "747649             chill sun although feel face get burnt  \n",
       "525499   violent feel toward bird outsid window shut w...  \n",
       "956297       get follow day use add everyon train pay vip  \n",
       "81013   head deposit check stop walmart get airsoft gu...  \n",
       "408310                         happi thought mum go fine   "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_samp.head() #seems good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71079dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_san = pd.read_pickle(\"./data/sanitized.pkl\")\n",
    "except FileNotFoundError:\n",
    "    print(\"No pickle file found, sanitizing existing df\")\n",
    "    \n",
    "    # Sanitize whole dataset\n",
    "    df[\"sanitized\"] = df[\"text\"].progress_apply(sanitize)\n",
    "\n",
    "    # Export it to pickle so we don't have to redo it\n",
    "    df.to_pickle(\"./data/sanitized.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bf5a92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1280000, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>emotion</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>date</th>\n",
       "      <th>lyx_query</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>sanitized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2063391019</td>\n",
       "      <td>Sun Jun 07 02:28:13 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>BerryGurus</td>\n",
       "      <td>@BreeMe more time to play with you BlackBerry ...</td>\n",
       "      <td>time play blackberri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2000525676</td>\n",
       "      <td>Mon Jun 01 22:18:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>peterlanoie</td>\n",
       "      <td>Failed attempt at booting to a flash drive. Th...</td>\n",
       "      <td>fail attempt boot flash drive fail attempt swi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2218180611</td>\n",
       "      <td>Wed Jun 17 22:01:38 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>will_tooker</td>\n",
       "      <td>@msproductions Well ain't that the truth. Wher...</td>\n",
       "      <td>well truth damn auto lock disabl go copi past ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2190269101</td>\n",
       "      <td>Tue Jun 16 02:14:47 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>sammutimer</td>\n",
       "      <td>@Meaghery cheers Craig - that was really sweet...</td>\n",
       "      <td>cheer craig realli sweet repli pump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2069249490</td>\n",
       "      <td>Sun Jun 07 15:31:58 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ohaijustin</td>\n",
       "      <td>I was reading the tweets that got send to me w...</td>\n",
       "      <td>read tweet got send lie phone face drop amp hi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  emotion    tweet_id                          date lyx_query  \\\n",
       "0   0        1  2063391019  Sun Jun 07 02:28:13 PDT 2009  NO_QUERY   \n",
       "1   1        0  2000525676  Mon Jun 01 22:18:53 PDT 2009  NO_QUERY   \n",
       "2   2        0  2218180611  Wed Jun 17 22:01:38 PDT 2009  NO_QUERY   \n",
       "3   3        1  2190269101  Tue Jun 16 02:14:47 PDT 2009  NO_QUERY   \n",
       "4   4        0  2069249490  Sun Jun 07 15:31:58 PDT 2009  NO_QUERY   \n",
       "\n",
       "          user                                               text  \\\n",
       "0   BerryGurus  @BreeMe more time to play with you BlackBerry ...   \n",
       "1  peterlanoie  Failed attempt at booting to a flash drive. Th...   \n",
       "2  will_tooker  @msproductions Well ain't that the truth. Wher...   \n",
       "3   sammutimer  @Meaghery cheers Craig - that was really sweet...   \n",
       "4   ohaijustin  I was reading the tweets that got send to me w...   \n",
       "\n",
       "                                           sanitized  \n",
       "0                               time play blackberri  \n",
       "1  fail attempt boot flash drive fail attempt swi...  \n",
       "2  well truth damn auto lock disabl go copi past ...  \n",
       "3               cheer craig realli sweet repli pump   \n",
       "4  read tweet got send lie phone face drop amp hi...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_san.shape)\n",
    "df_san.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0ab5135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before sanitizing 18398298\n",
      "After sanitizing 9575942\n"
     ]
    }
   ],
   "source": [
    "print(\"Before sanitizing\", df['text'].apply(lambda x: len(x.split(' '))).sum())\n",
    "print(\"After sanitizing\", df_san['sanitized'].apply(lambda x: len(x.split(' '))).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b25f6ea",
   "metadata": {},
   "source": [
    "Just to check, we see that before sanitizing, we had 18'398'298 words. We were able to halve it to 9'575'942 by sanitization and stemming our tweets.\n",
    "\n",
    "Before fitting our models, we also take a subsample to be able to compute them faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "036e6d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Id 423388\n",
      "Last Id 503594\n",
      "Length 192000\n"
     ]
    }
   ],
   "source": [
    "df_sub = df_san.sample(frac=0.15, random_state=SEED)\n",
    "\n",
    "# To check reproducibility\n",
    "print(\"First Id\", df_sub[\"Id\"].iloc[0])\n",
    "print(\"Last Id\", df_sub[\"Id\"].iloc[-1])\n",
    "print(\"Length\", df_sub.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cc7c1c",
   "metadata": {},
   "source": [
    "# 2. Fitting\n",
    "\n",
    "Now that we have a subsample (15%) of our cleaned data, we can try to fit some models to see what it gives us. We also define a standard 10 kfolds which we will use for our cross-validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00606ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_sub[\"sanitized\"], df_sub[\"emotion\"], \n",
    "                                                    test_size=0.2, shuffle=True, random_state=SEED)\n",
    "folds = KFold(n_splits=10, shuffle=True, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0434f5eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  (153600,)\n",
      "X_test:  (38400,)\n",
      "y_train:  (153600,)\n",
      "y_test:  (38400,)\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "print(\"X_train: \", X_train.shape)\n",
    "print(\"X_test: \", X_test.shape)\n",
    "print(\"y_train: \", y_train.shape)\n",
    "print(\"y_test: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998f67ac",
   "metadata": {},
   "source": [
    "First model we will try is the `BernoulliNB` since we have binary data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "54fcc643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 7.663532018661499\n",
      "Mean CV accuracy: 0.7543294270833332\n",
      "Test accuracy: 0.7544010416666667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nTime 8.715646505355835\\nMean CV accuracy: 0.7543294270833332\\nTest accuracy: 0.7544010416666667\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "berNB = Pipeline(\n",
    "    [\n",
    "        (\"tfidf\", TfidfVectorizer()),\n",
    "        (\"clf\", BernoulliNB()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "CV_ber = cross_val_score(\n",
    "    berNB, X_train, y_train, scoring=\"accuracy\", cv=folds, n_jobs=-1\n",
    ")\n",
    "\n",
    "berNB.fit(X_train, y_train)\n",
    "y_pred = berNB.predict(X_test)\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "print(f\"Time {time.time() - start}\")\n",
    "print(f\"Mean CV accuracy: {np.mean(CV_ber)}\")\n",
    "print(f\"Test accuracy: {score}\")\n",
    "\n",
    "\"\"\"\n",
    "Time 8.715646505355835\n",
    "Mean CV accuracy: 0.7543294270833332\n",
    "Test accuracy: 0.7544010416666667\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0b2478",
   "metadata": {},
   "source": [
    "Without any paramter tuning, we got 75% with the naive Bayes Bernoulli classifier.\n",
    "Baseline is `0.77309`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a631e089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 126.00729584693909\n",
      "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
      "18       1.008442      0.034566         0.211298        0.011747   \n",
      "\n",
      "   param_clf__alpha param_clf__fit_prior  \\\n",
      "18              5.0                False   \n",
      "\n",
      "                                          params  split0_test_score  \\\n",
      "18  {'clf__alpha': 5.0, 'clf__fit_prior': False}           0.757617   \n",
      "\n",
      "    split1_test_score  split2_test_score  split3_test_score  \\\n",
      "18           0.756022           0.757682           0.755339   \n",
      "\n",
      "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
      "18           0.756966         0.756725        0.000915                1  \n"
     ]
    }
   ],
   "source": [
    "berNB = Pipeline(\n",
    "    [\n",
    "        (\"vect\", CountVectorizer()),\n",
    "        (\"tfidf\", TfidfTransformer()),\n",
    "        (\"clf\", BernoulliNB()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "gridsearch  = GridSearchCV(\n",
    "    berNB,\n",
    "    {\n",
    "        \"clf__alpha\": np.linspace(1e-10, 5, 10),\n",
    "        \"clf__fit_prior\": [False, True]\n",
    "    },\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "gridsearch.fit(X_train, y_train)\n",
    "res = gridsearch.cv_results_\n",
    "resdf = pd.DataFrame(res)\n",
    "print(f\"Time {time.time() - start}\")\n",
    "print(resdf[resdf[\"rank_test_score\"] == 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c03529",
   "metadata": {},
   "source": [
    "No matter the alpha, we don't seem to get higher than 0.75. BernoulliNB is not the best model here.\n",
    "Instead of having to write the same for every model, let's try to automatize the testing just by specifying the model and the parameters grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0fab597e",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [BernoulliNB(), LogisticRegression(warm_start=True)]\n",
    "models = [LogisticRegression(warm_start=True)]\n",
    "\n",
    "params_tfid = {\n",
    "    \"tfidfvectorizer__norm\": [\"l1\", \"l2\"],\n",
    "    #\"tfidfvectorizer__use_idf\": [True, False],\n",
    "    #\"tfidfvectorizer__smooth_idf\": [True, False],\n",
    "    #\"tfidfvectorizer__sublinear_tf\": [True, False]\n",
    "}\n",
    "\n",
    "params = {\n",
    "    \"bernoullinb\": {\n",
    "        \"bernoullinb__alpha\": np.linspace(1e-10, 10, 5),\n",
    "        \"bernoullinb__fit_prior\": [False, True],\n",
    "    },\n",
    "    \"logisticregression\": {\n",
    "        \"logisticregression__penalty\": [\"l1\", \"l2\", \"elasticnet\"],\n",
    "        #\"logisticregression__dual\": [False, True], try with liblinear\n",
    "        \"logisticregression__C\": 10**np.linspace(-3, 2, 6),\n",
    "        \"logisticregression__random_state\": [SEED],\n",
    "        #\"logisticregression__solver\": [\"newton-cg\", \"lbfgs\", \"saga\"],\n",
    "        \"logisticregression__solver\": [\"saga\"],\n",
    "        \"logisticregression__l1_ratio\": np.linspace(0, 1, 5),\n",
    "    }\n",
    "}\n",
    "\n",
    "# If we also want to gridsearch the different Tfidf params\n",
    "for k, v in params_tfid.items():\n",
    "    params[\"bernoullinb\"][k] = v\n",
    "    #Easier if we comment above\n",
    "    pass\n",
    "\n",
    "pipes = []\n",
    "\n",
    "# Also check what we can do with the TfidfVectorizer parameters\n",
    "for model in models:\n",
    "    pipe = make_pipeline(TfidfVectorizer(), model)\n",
    "    pipes.append(pipe)\n",
    "    \n",
    "    # Will use that once we have the best params\n",
    "    #pipe.set_params(**params[pipe.steps[1][0]])\n",
    "\n",
    "# Initialize empty dictionary\n",
    "reports = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460f51e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " logisticregression\n",
      "Fitting 5 folds for each of 360 candidates, totalling 1800 fits\n"
     ]
    }
   ],
   "source": [
    "# Fit each different pipeline\n",
    "\n",
    "for pipe in pipes:\n",
    "    print(\"\\n\", pipe.steps[1][0])\n",
    "    start = time.time()\n",
    "    \n",
    "    gridsearch = GridSearchCV(pipe, params[pipe.steps[1][0]], scoring=\"accuracy\", cv=folds, n_jobs=-1, verbose=3)\n",
    "    gridsearch.fit(X_train, y_train)\n",
    "    y_pred = gridsearch.predict(X_test)\n",
    "    \n",
    "    score = accuracy_score(y_test, y_pred)\n",
    "    resdf = pd.DataFrame(gridsearch.cv_results_)\n",
    "    \n",
    "    reports[pipe.steps[1][0]] = classification_report(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Time {time.time() - start}s\")\n",
    "    print(resdf[resdf[\"rank_test_score\"] == 1])\n",
    "    print(f\"Test accuracy: {score}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b8c584",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "BernoulliNB() - 0.76\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a1150e89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bernoullinb': '              precision    recall  f1-score   support\\n\\n           0       0.76      0.76      0.76     19173\\n           1       0.76      0.76      0.76     19227\\n\\n    accuracy                           0.76     38400\\n   macro avg       0.76      0.76      0.76     38400\\nweighted avg       0.76      0.76      0.76     38400\\n'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reports"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
