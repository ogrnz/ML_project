{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fd1a342",
   "metadata": {},
   "source": [
    "## ML competition\n",
    "\n",
    "_Marilyn, Shiva, Olivier_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62e7985e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c7050736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup chunk\n",
    "\n",
    "import time\n",
    "\n",
    "# Custom utils\n",
    "from utils import *\n",
    "\n",
    "# Data wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Text sanitization\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "try:\n",
    "    # Avoid error if you don't have the resource\n",
    "    stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "except LookupError:\n",
    "    nltk.download(\"stopwords\")\n",
    "    stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "    \n",
    "stemmer = SnowballStemmer(language=\"english\")\n",
    "\n",
    "# Lang detection\n",
    "#import langid\n",
    "#from langid.langid import LanguageIdentifier, model\n",
    "#identifier = LanguageIdentifier.from_modelstring(model, norm_probs=True)\n",
    "\n",
    "# Misc\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# Define the seed for reproducibility\n",
    "SEED = 31415\n",
    "# Define n_jobs\n",
    "JOBS = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ff07f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit time\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import (\n",
    "    LinearDiscriminantAnalysis,\n",
    "    QuadraticDiscriminantAnalysis,\n",
    ")\n",
    "\n",
    "from sklearn.feature_extraction.text import (\n",
    "    CountVectorizer, \n",
    "    TfidfTransformer, \n",
    "    TfidfVectorizer\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, \n",
    "    GridSearchCV, \n",
    "    KFold, \n",
    "    cross_val_score\n",
    ")\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    accuracy_score, \n",
    "    confusion_matrix\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bd3e8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/MLUnige2021_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9723ea70",
   "metadata": {},
   "source": [
    "### 1. EDA\n",
    "Small EDA to check a bit the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43c9ce7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1280000 entries, 0 to 1279999\n",
      "Data columns (total 7 columns):\n",
      "Id           1280000 non-null int64\n",
      "emotion      1280000 non-null int64\n",
      "tweet_id     1280000 non-null int64\n",
      "date         1280000 non-null object\n",
      "lyx_query    1280000 non-null object\n",
      "user         1280000 non-null object\n",
      "text         1280000 non-null object\n",
      "dtypes: int64(3), object(4)\n",
      "memory usage: 68.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6eaa09",
   "metadata": {},
   "source": [
    "More than a million entries. What is `lyx_query`? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24fad17d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NO_QUERY'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"lyx_query\"].head()\n",
    "df[\"lyx_query\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b5ea6e",
   "metadata": {},
   "source": [
    "Welp only `\"NO_QUERY\"` so we can drop it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45e1d2b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          2063391019\n",
       "1          2000525676\n",
       "2          2218180611\n",
       "3          2190269101\n",
       "4          2069249490\n",
       "              ...    \n",
       "1279995    1835296397\n",
       "1279996    2226720395\n",
       "1279997    1962176213\n",
       "1279998    1976894947\n",
       "1279999    1563596981\n",
       "Name: tweet_id, Length: 1280000, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"tweet_id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a499623",
   "metadata": {},
   "source": [
    "Those are old tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd770a52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          Sun Jun 07 02:28:13 PDT 2009\n",
       "1          Mon Jun 01 22:18:53 PDT 2009\n",
       "2          Wed Jun 17 22:01:38 PDT 2009\n",
       "3          Tue Jun 16 02:14:47 PDT 2009\n",
       "4          Sun Jun 07 15:31:58 PDT 2009\n",
       "                       ...             \n",
       "1279995    Mon May 18 05:39:18 PDT 2009\n",
       "1279996    Thu Jun 18 12:18:05 PDT 2009\n",
       "1279997    Fri May 29 10:38:30 PDT 2009\n",
       "1279998    Sat May 30 19:28:13 PDT 2009\n",
       "1279999    Sun Apr 19 23:27:25 PDT 2009\n",
       "Name: date, Length: 1280000, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"date\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a273ba1e",
   "metadata": {},
   "source": [
    "Indeed they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "480e15c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "574114"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[\"user\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e94d2c",
   "metadata": {},
   "source": [
    "Lots of different users. If we had only like 1000s of users, we could have looked for some pattern (user X is only positive ...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e8dacbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emotion                                                    1\n",
       "text       @BreeMe more time to play with you BlackBerry ...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0, [\"emotion\", \"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "add282fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"emotion\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5ab453e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "236"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"emotion\"]==1].shape[0] - df[df[\"emotion\"]==0].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad634686",
   "metadata": {},
   "source": [
    "Perfectly balanced dataset (236 diff between the 2 classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd5dbee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.bar([\"negative\", \"positive\"], [df[df[\"emotion\"]==0].shape[0], df[df[\"emotion\"]==1].shape[0]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431f384b",
   "metadata": {},
   "source": [
    "Let's also check the language of the tweets (all eng or also others?). For that, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d21f3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_samp = df.sample(150_000) #Total df takes 1h to run\n",
    "\n",
    "# Commented because we don't want it to run everytime\n",
    "#df_samp[\"lang\"] = df_samp[\"text\"].progress_apply(lang_detect)\n",
    "#df_samp[\"lang\"].unique()\n",
    "#df_samp[~(df_samp[\"lang\"] == \"en\")].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464f68b3",
   "metadata": {},
   "source": [
    "Only 15k tweets which are not detected as english in our 150k sample. By checking some of the tweets, most are english, but the language detector surely has some trouble with some very short tweets containing one or more foreign words.--> consider all as english!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972f0c10",
   "metadata": {},
   "source": [
    "## 2. Strategy\n",
    "\n",
    "1. Preprocess the text\n",
    "    1. remove punctuation marks\n",
    "    2. remove stopwords (en)\n",
    "    3. stem or lemmatize the words\n",
    "    \n",
    "    \n",
    "2. take a sample of our whole dataset (200k?) to do our preliminary test. We can't do cross validation on the whole dataset.\n",
    "\n",
    "3. Begin to fit the models. \n",
    "    1. Pipeline with TfidfTransformer (or the other one I don't remember the name)\n",
    "    2. BerNB\n",
    "    3. LogisticRegression\n",
    "    4. RidgeClassifier\n",
    "    5. SGDClassifier\n",
    "    6. SVC\n",
    "    7. RandomForestClassifier\n",
    "    8. DecisionTreeClassifier\n",
    "    9. KNeighborsClassifier\n",
    "    10. LDA? QDA?\n",
    "4. Select a few of the best models, CV with bigger dataset\n",
    "5. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8141c212",
   "metadata": {},
   "source": [
    "## Misc\n",
    "\n",
    "- Should pay attention to some special chars which were not removed: \"I \\&lt;3 you\" (#2204), at first just remove it, but we could try investigate that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "d0a8af77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7082"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"text\"].str.contains(\"&lt;3\")].count()\n",
    "(df[df[\"text\"].str.contains(\"&lt;3\")].loc[:, \"emotion\"] == 1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3514dc5b",
   "metadata": {},
   "source": [
    "10635 tweets have that, don't think it's worth it. And out of those, only 7082 are positive. It's not even as sure as that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14ad713",
   "metadata": {},
   "source": [
    "# 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a87d5ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanitized: hello shiva marilyn corona \n"
     ]
    }
   ],
   "source": [
    "# Some special strings to test \n",
    "txt1 = \"Hello @Shiva and @Marilyn! https://hello.ch 12334 #corona ...\"\n",
    "print(\"Sanitized:\", sanitize(txt1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1223347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>emotion</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>date</th>\n",
       "      <th>lyx_query</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1168603</th>\n",
       "      <td>1168603</td>\n",
       "      <td>1</td>\n",
       "      <td>2001190216</td>\n",
       "      <td>Tue Jun 02 00:06:31 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bonnietsang</td>\n",
       "      <td>@AdamandEveWed Thanks! Are you going to Sprink...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017561</th>\n",
       "      <td>1017561</td>\n",
       "      <td>0</td>\n",
       "      <td>2062884238</td>\n",
       "      <td>Sun Jun 07 00:37:25 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TennealMaree</td>\n",
       "      <td>i want the sims 3  stupid indian man killed my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651245</th>\n",
       "      <td>651245</td>\n",
       "      <td>0</td>\n",
       "      <td>2049892047</td>\n",
       "      <td>Fri Jun 05 17:41:12 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>sssallypau</td>\n",
       "      <td>carnival was fun! scratched up knees isn't</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950878</th>\n",
       "      <td>950878</td>\n",
       "      <td>0</td>\n",
       "      <td>2235937462</td>\n",
       "      <td>Fri Jun 19 02:27:03 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>nautynature</td>\n",
       "      <td>@aaroncarter7 i called u a king and legend and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688158</th>\n",
       "      <td>688158</td>\n",
       "      <td>1</td>\n",
       "      <td>2067742010</td>\n",
       "      <td>Sun Jun 07 12:52:48 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>xoAlexHeartsxo</td>\n",
       "      <td>@David_DB I'm great thanks, just heading to be...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Id  emotion    tweet_id                          date lyx_query  \\\n",
       "1168603  1168603        1  2001190216  Tue Jun 02 00:06:31 PDT 2009  NO_QUERY   \n",
       "1017561  1017561        0  2062884238  Sun Jun 07 00:37:25 PDT 2009  NO_QUERY   \n",
       "651245    651245        0  2049892047  Fri Jun 05 17:41:12 PDT 2009  NO_QUERY   \n",
       "950878    950878        0  2235937462  Fri Jun 19 02:27:03 PDT 2009  NO_QUERY   \n",
       "688158    688158        1  2067742010  Sun Jun 07 12:52:48 PDT 2009  NO_QUERY   \n",
       "\n",
       "                   user                                               text  \n",
       "1168603     bonnietsang  @AdamandEveWed Thanks! Are you going to Sprink...  \n",
       "1017561    TennealMaree  i want the sims 3  stupid indian man killed my...  \n",
       "651245       sssallypau        carnival was fun! scratched up knees isn't   \n",
       "950878      nautynature  @aaroncarter7 i called u a king and legend and...  \n",
       "688158   xoAlexHeartsxo  @David_DB I'm great thanks, just heading to be...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As a test, sanitize the subsample\n",
    "#df_samp[\"sanitized\"] = df_samp[\"text\"].progress_apply(sanitize)\n",
    "df_samp.head() #seems good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "45354e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    #If you have an error with sanitized-rec.pkl try to take sanitized.pkl\n",
    "    df_san = pd.read_pickle(\"./data/sanitized-rec.pkl\")\n",
    "except FileNotFoundError:\n",
    "    print(\"No pickle file found, sanitizing existing df\")\n",
    "    \n",
    "    # Sanitize whole dataset\n",
    "    df_san = df.copy()\n",
    "    df_san[\"sanitized\"] = df[\"text\"].progress_apply(sanitize)\n",
    "\n",
    "    # Export it to pickle so we don't have to redo it\n",
    "    df_san.to_pickle(\"./data/sanitized.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a220634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1280000, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>emotion</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>date</th>\n",
       "      <th>lyx_query</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>sanitized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2063391019</td>\n",
       "      <td>Sun Jun 07 02:28:13 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>BerryGurus</td>\n",
       "      <td>@BreeMe more time to play with you BlackBerry ...</td>\n",
       "      <td>breem time play blackberri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2000525676</td>\n",
       "      <td>Mon Jun 01 22:18:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>peterlanoie</td>\n",
       "      <td>Failed attempt at booting to a flash drive. Th...</td>\n",
       "      <td>fail attempt boot flash drive fail attempt swi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2218180611</td>\n",
       "      <td>Wed Jun 17 22:01:38 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>will_tooker</td>\n",
       "      <td>@msproductions Well ain't that the truth. Wher...</td>\n",
       "      <td>msproduct well truth damn auto lock disabl go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2190269101</td>\n",
       "      <td>Tue Jun 16 02:14:47 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>sammutimer</td>\n",
       "      <td>@Meaghery cheers Craig - that was really sweet...</td>\n",
       "      <td>meagheri cheer craig realli sweet repli pump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2069249490</td>\n",
       "      <td>Sun Jun 07 15:31:58 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ohaijustin</td>\n",
       "      <td>I was reading the tweets that got send to me w...</td>\n",
       "      <td>read tweet got send lie phone face drop amp hi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  emotion    tweet_id                          date lyx_query  \\\n",
       "0   0        1  2063391019  Sun Jun 07 02:28:13 PDT 2009  NO_QUERY   \n",
       "1   1        0  2000525676  Mon Jun 01 22:18:53 PDT 2009  NO_QUERY   \n",
       "2   2        0  2218180611  Wed Jun 17 22:01:38 PDT 2009  NO_QUERY   \n",
       "3   3        1  2190269101  Tue Jun 16 02:14:47 PDT 2009  NO_QUERY   \n",
       "4   4        0  2069249490  Sun Jun 07 15:31:58 PDT 2009  NO_QUERY   \n",
       "\n",
       "          user                                               text  \\\n",
       "0   BerryGurus  @BreeMe more time to play with you BlackBerry ...   \n",
       "1  peterlanoie  Failed attempt at booting to a flash drive. Th...   \n",
       "2  will_tooker  @msproductions Well ain't that the truth. Wher...   \n",
       "3   sammutimer  @Meaghery cheers Craig - that was really sweet...   \n",
       "4   ohaijustin  I was reading the tweets that got send to me w...   \n",
       "\n",
       "                                           sanitized  \n",
       "0                         breem time play blackberri  \n",
       "1  fail attempt boot flash drive fail attempt swi...  \n",
       "2   msproduct well truth damn auto lock disabl go...  \n",
       "3      meagheri cheer craig realli sweet repli pump   \n",
       "4  read tweet got send lie phone face drop amp hi...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_san.shape)\n",
    "df_san.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c14ff1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before sanitizing 18398298\n",
      "After sanitizing 10730388\n"
     ]
    }
   ],
   "source": [
    "print(\"Before sanitizing\", df['text'].apply(lambda x: len(x.split(' '))).sum())\n",
    "print(\"After sanitizing\", df_san['sanitized'].apply(lambda x: len(x.split(' '))).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b257a321",
   "metadata": {},
   "source": [
    "Just to check, we see that before sanitizing, we had 18'398'298 words. We were able to halve it to 9'575'942 by sanitization and stemming our tweets.\n",
    "\n",
    "Before fitting our models, we also take a subsample to be able to compute them faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5a80b9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Id 423388\n",
      "Last Id 813069\n",
      "Length 1280000\n"
     ]
    }
   ],
   "source": [
    "df_sub = df_san.sample(frac=1, random_state=SEED)\n",
    "\n",
    "# To check reproducibility\n",
    "print(\"First Id\", df_sub[\"Id\"].iloc[0])\n",
    "print(\"Last Id\", df_sub[\"Id\"].iloc[-1])\n",
    "print(\"Length\", df_sub.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77973460",
   "metadata": {},
   "source": [
    "# 2. Fitting\n",
    "\n",
    "Now that we have a subsample (10%) of our cleaned data, we can try to fit some models to see what it gives us. We also define a standard 10 kfolds which we will use for our cross-validation. \n",
    "\n",
    "Just as a note to justify the 10% choice for our sample. The bernoulli classifier below achieves, with default settings, an accuracy score of `76.37%` when using the whole dataset. For 10% of the dataset, the accuracy drops to `75.24%`. A 1% drop in accuracy for 10 times less computation time seems worth it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "289a928a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_sub[\"sanitized\"], df_sub[\"emotion\"], \n",
    "                                                    test_size=0.2, shuffle=True, random_state=SEED)\n",
    "\n",
    "#only 4 folds because I have 4 cores, just to test\n",
    "folds = KFold(n_splits=4, shuffle=True, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "87e548b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  (1024000,)\n",
      "X_test:  (256000,)\n",
      "y_train:  (1024000,)\n",
      "y_test:  (256000,)\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "print(\"X_train: \", X_train.shape)\n",
    "print(\"X_test: \", X_test.shape)\n",
    "print(\"y_train: \", y_train.shape)\n",
    "print(\"y_test: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbeb5ccc",
   "metadata": {},
   "source": [
    "First model we will try is the `BernoulliNB` since we have binary data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6fae0cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 45.136425495147705\n",
      "Mean CV accuracy: 0.7648857421875\n",
      "Test accuracy: 0.76526953125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nWhole dataset:\\nTime 21.568854093551636\\nMean CV accuracy: 0.7639306640625\\nTest accuracy: 0.76375390625\\n\\n10% sample:\\nTime 2.146036386489868\\nMean CV accuracy: 0.7483203125\\nTest accuracy: 0.7523828125\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "berNB = Pipeline(\n",
    "    [\n",
    "        (\"tfidf\", TfidfVectorizer()),\n",
    "        (\"clf\", BernoulliNB()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "CV_ber = cross_val_score(\n",
    "    berNB, X_train, y_train, scoring=\"accuracy\", cv=folds, n_jobs=JOBS\n",
    ")\n",
    "\n",
    "berNB.fit(X_train, y_train)\n",
    "y_pred = berNB.predict(X_test)\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "print(f\"Time {time.time() - start}\")\n",
    "print(f\"Mean CV accuracy: {np.mean(CV_ber)}\")\n",
    "print(f\"Test accuracy: {score}\")\n",
    "\n",
    "\"\"\"\n",
    "Whole dataset:\n",
    "Time 21.568854093551636\n",
    "Mean CV accuracy: 0.7639306640625\n",
    "Test accuracy: 0.76375390625\n",
    "\n",
    "10% sample:\n",
    "Time 2.146036386489868\n",
    "Mean CV accuracy: 0.7483203125\n",
    "Test accuracy: 0.7523828125\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed550e61",
   "metadata": {},
   "source": [
    "Without any paramter tuning, we got 75% with the naive Bayes Bernoulli classifier.\n",
    "Baseline is `0.77309`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4acf60",
   "metadata": {},
   "source": [
    "No matter the alpha, we don't seem to get higher than 0.75. BernoulliNB is not the best model here.\n",
    "Instead of having to write the same for every model, let's try to automatize the testing just by specifying the model and the parameters grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ceefb87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models = [LogisticRegression(warm_start=True)]\n",
    "models = [SGDClassifier(warm_start=True)]\n",
    "models = [BaggingClassifier(warm_start=True)]\n",
    "models = [RandomForestClassifier(warm_start=True)]\n",
    "models = [BaggingClassifier(warm_start=True)]\n",
    "models = [SVC()]\n",
    "models = [LinearSVC()]\n",
    "models = [RidgeClassifier()]\n",
    "models = [BernoulliNB()]\n",
    "\n",
    "params_tfid = {\n",
    "    \"tfidfvectorizer__norm\": [\"l2\"],\n",
    "    \"tfidfvectorizer__analyzer\": [\"word\"],\n",
    "    \"tfidfvectorizer__ngram_range\": [(1,2)],\n",
    "    \"tfidfvectorizer__max_df\": [0.7],\n",
    "    \"tfidfvectorizer__min_df\": [3],\n",
    "    \"tfidfvectorizer__binary\": [True, False],\n",
    "    #\"tfidfvectorizer__use_idf\": [True, False],\n",
    "    #\"tfidfvectorizer__smooth_idf\": [True, False],\n",
    "    #\"tfidfvectorizer__sublinear_tf\": [True, False]\n",
    "}\n",
    "\n",
    "params = {\n",
    "    \"bernoullinb\": {\n",
    "        \"bernoullinb__alpha\": [7.5],\n",
    "        \"bernoullinb__fit_prior\": [False, True],\n",
    "    },\n",
    "    \"ridgeclassifier\": {\n",
    "        \"ridgeclassifier__alpha\": np.linspace(1e-5, 10, 5),\n",
    "        \"ridgeclassifier__class_weight\": [\"balanced\", None],\n",
    "        \"ridgeclassifier__normalize\": [False, True],\n",
    "        \n",
    "    },\n",
    "    \"logisticregression\": {\n",
    "        #\"logisticregression__penalty\": [\"l1\", \"l2\", \"elasticnet\"],\n",
    "        \"logisticregression__penalty\": [\"l1\"],\n",
    "        #\"logisticregression__dual\": [False, True], try with liblinear\n",
    "        \"logisticregression__C\": 10**np.linspace(-3, -0.001, 4),\n",
    "        #\"logisticregression__C\": [1e-1],\n",
    "        \"logisticregression__random_state\": [SEED],\n",
    "        #\"logisticregression__solver\": [\"newton-cg\", \"lbfgs\", \"saga\"],\n",
    "        \"logisticregression__solver\": [\"saga\"],\n",
    "        #\"logisticregression__l1_ratio\": np.linspace(0.1, 0.9, 5),\n",
    "    },\n",
    "    \"sgdclassifier\": {\n",
    "        \"sgdclassifier__random_state\": [SEED],\n",
    "        \"sgdclassifier__loss\": [\"modified_huber\"],\n",
    "        \"sgdclassifier__alpha\": 10**np.linspace(-3, -0.001, 4),\n",
    "    },\n",
    "    \"baggingclassifier\": {\n",
    "        \"baggingclassifier__random_state\": [SEED],\n",
    "        \"baggingclassifier__n_estimators\": [30],\n",
    "        \"baggingclassifier__max_samples\": [0.05],\n",
    "        \"baggingclassifier__max_features\": [0.5],\n",
    "        \n",
    "    },\n",
    "    \"randomforestclassifier\": {\n",
    "        \"randomforestclassifier__random_state\": [SEED],\n",
    "    },\n",
    "    \"svc\": {\n",
    "        \"svc__random_state\": [SEED],\n",
    "    },\n",
    "    \"linearsvc\": {\n",
    "        \"linearsvc__random_state\": [SEED],\n",
    "        \"linearsvc__loss\": [\"squared_hinge\"],\n",
    "        \"linearsvc__penalty\": [\"l2\"],\n",
    "        \"linearsvc__max_iter\": [1000],\n",
    "        \"linearsvc__dual\": [False],\n",
    "        \"linearsvc__C\": np.linspace(0.01, 0.2, 10),\n",
    "        \"linearsvc__class_weight\": [\"balanced\"],\n",
    "    }\n",
    "}\n",
    "\n",
    "# If we also want to gridsearch the different Tfidf params\n",
    "for k, v in params_tfid.items():\n",
    "    params[\"bernoullinb\"][k] = v\n",
    "    #Easier if we comment above\n",
    "    pass\n",
    "\n",
    "pipes = []\n",
    "\n",
    "# Also check what we can do with the TfidfVectorizer parameters\n",
    "for model in models:\n",
    "    pipe = make_pipeline(TfidfVectorizer(), model)\n",
    "    pipes.append(pipe)\n",
    "    \n",
    "    # Will use that once we have the best params\n",
    "    #pipe.set_params(**params[pipe.steps[1][0]])\n",
    "\n",
    "# Initialize empty dictionary\n",
    "reports = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "92ecba43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00000000e-05, 3.68600191e-05, 1.35866101e-04, 5.00802706e-04,\n",
       "       1.84595973e-03, 6.80421108e-03, 2.50803350e-02, 9.24461627e-02,\n",
       "       3.40756732e-01, 1.25602996e+00])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10**np.linspace(-5, 0.099, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "e3668a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bernoullinb\n",
      "Fitting 4 folds for each of 2 candidates, totalling 8 fits\n",
      "Time 3.202148675918579s\n",
      "Test accuracy: 0.7553125\n"
     ]
    }
   ],
   "source": [
    "# Fit each different pipeline\n",
    "\n",
    "for pipe in pipes:\n",
    "    print(pipe.steps[1][0])\n",
    "    start = time.time()\n",
    "    \n",
    "    gridsearch = GridSearchCV(pipe, params[pipe.steps[1][0]], scoring=\"accuracy\", cv=folds, n_jobs=JOBS, verbose=3)\n",
    "    gridsearch.fit(X_train, y_train)\n",
    "    y_pred = gridsearch.predict(X_test)\n",
    "    \n",
    "    score = accuracy_score(y_test, y_pred)\n",
    "    resdf = pd.DataFrame(gridsearch.cv_results_)\n",
    "    \n",
    "    reports[pipe.steps[1][0]] = classification_report(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Time {time.time() - start}s\")\n",
    "    #print(resdf[resdf[\"rank_test_score\"] == 1])\n",
    "    print(f\"Test accuracy: {score}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "edb6739b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_bernoullinb__alpha</th>\n",
       "      <th>param_bernoullinb__fit_prior</th>\n",
       "      <th>param_tfidfvectorizer__analyzer</th>\n",
       "      <th>param_tfidfvectorizer__norm</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.686249</td>\n",
       "      <td>0.014220</td>\n",
       "      <td>0.224001</td>\n",
       "      <td>0.020384</td>\n",
       "      <td>7.5</td>\n",
       "      <td>False</td>\n",
       "      <td>word</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'bernoullinb__alpha': 7.5, 'bernoullinb__fit_...</td>\n",
       "      <td>0.749570</td>\n",
       "      <td>0.753828</td>\n",
       "      <td>0.753437</td>\n",
       "      <td>0.756953</td>\n",
       "      <td>0.753447</td>\n",
       "      <td>0.00262</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.663750</td>\n",
       "      <td>0.016436</td>\n",
       "      <td>0.193498</td>\n",
       "      <td>0.006983</td>\n",
       "      <td>7.5</td>\n",
       "      <td>True</td>\n",
       "      <td>word</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'bernoullinb__alpha': 7.5, 'bernoullinb__fit_...</td>\n",
       "      <td>0.749844</td>\n",
       "      <td>0.754023</td>\n",
       "      <td>0.752734</td>\n",
       "      <td>0.756563</td>\n",
       "      <td>0.753291</td>\n",
       "      <td>0.00242</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       0.686249      0.014220         0.224001        0.020384   \n",
       "1       0.663750      0.016436         0.193498        0.006983   \n",
       "\n",
       "  param_bernoullinb__alpha param_bernoullinb__fit_prior  \\\n",
       "0                      7.5                        False   \n",
       "1                      7.5                         True   \n",
       "\n",
       "  param_tfidfvectorizer__analyzer param_tfidfvectorizer__norm  \\\n",
       "0                            word                          l2   \n",
       "1                            word                          l2   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "0  {'bernoullinb__alpha': 7.5, 'bernoullinb__fit_...           0.749570   \n",
       "1  {'bernoullinb__alpha': 7.5, 'bernoullinb__fit_...           0.749844   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  mean_test_score  \\\n",
       "0           0.753828           0.753437           0.756953         0.753447   \n",
       "1           0.754023           0.752734           0.756563         0.753291   \n",
       "\n",
       "   std_test_score  rank_test_score  \n",
       "0         0.00262                1  \n",
       "1         0.00242                2  "
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resdf.sort_values(by=[\"rank_test_score\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6f136d",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "Linear models: <br>\n",
    "BernoulliNB: 0.76 <br>\n",
    "LogisticRegression: <br>\n",
    "    l2 - 0.76 <br>\n",
    "    l2, C range - 0.76 <br>\n",
    "    elasticnet, C range, l1_ratio range - 0.75 <br>\n",
    "    l1, C=0.1 - 0.74 <br>\n",
    "    l1, C range - 0.76 <br>\n",
    "sgdclassifier: <br>\n",
    "    all default - 0.75 <br>\n",
    "    modified_huber - 0.75 <br>\n",
    "ridgeclassifier: <br>\n",
    "    alpha=7.5, class_weight=balanced, normalize=False -> 0.76\n",
    "    \n",
    "Linear models seem not to perform better than around 0.75. Let's go to ensembles.\n",
    "\n",
    "Ensemble: <br>\n",
    "RandomForestClassifier: <br>\n",
    "    max_samples 0.001 - 0.59 <br>\n",
    "    max_samples 0.1 - 0.71 <br>\n",
    "    max_samples 0.8 - 0.73 <br>\n",
    "    max_samples 0.01, n_estimators 40 - 0.71 <br>\n",
    "    max_samples 0.01, n_estimators 20 - 0.68 <br>\n",
    "    max_samples 0.01, n_estimators 100 - 0.72 <br>\n",
    "    max_samples 0.1, n_estimators 50 - 0.74 <br>\n",
    "    max_samples 0.5, n_estimators 70 - 0.75 <br>\n",
    "    max_samples 0.1, n_estimators 150 - 0.75 <br>\n",
    "\n",
    "BaggingClassifier: <br>\n",
    "    max_samples 0.01, n_estimators 100 - 0.70 <br>\n",
    "    max_samples 0.1, n_estimators 10 - 0.72 <br>\n",
    "    max_samples 0.05, n_estimators 30 - 0.72 <br>\n",
    "    max_samples 0.05, n_estimators 30, max_features 0.5 - 0.72 <br>\n",
    "    \n",
    "SVC: -> not fast enough\n",
    " \n",
    "LinearSVC: <br>\n",
    "default: 0.75 -> really fast! <br>\n",
    "squared_hinge + dual=False + C=0.1 -> 0.76 <br>\n",
    "squared_hinge + dual=False + C=0.1 + class_weight=balanced -> 0.76\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea653dd",
   "metadata": {},
   "source": [
    "No matter the classifier, we don't seem to go above 76%. Look into the data to see if we can see something.\n",
    "-> delete very short tweets?\n",
    "-> lemmatization?\n",
    "\n",
    "Maybe deleted too much noise during preprocessing, only remove stopwords? (+URLs?)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
