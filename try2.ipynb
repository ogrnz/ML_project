{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fde2d11",
   "metadata": {},
   "source": [
    "## ML competition\n",
    "### Try 2\n",
    "\n",
    "_Marilyn, Shiva, Olivier_\n",
    "\n",
    "The goal of this file is to try some different (less restrictive) sanitization procedures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bb87c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1bf9dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup chunk\n",
    "\n",
    "import time\n",
    "\n",
    "# Custom utils\n",
    "from utils import *\n",
    "\n",
    "# Data wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Text sanitization\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "try:\n",
    "    # Avoid error if you don't have the resource\n",
    "    stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "except LookupError:\n",
    "    nltk.download(\"stopwords\")\n",
    "    stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "    \n",
    "stemmer = SnowballStemmer(language=\"english\")\n",
    "\n",
    "# Lang detection\n",
    "#import langid\n",
    "#from langid.langid import LanguageIdentifier, model\n",
    "#identifier = LanguageIdentifier.from_modelstring(model, norm_probs=True)\n",
    "\n",
    "# Misc\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# Define the seed for reproducibility\n",
    "SEED = 31415"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13efccc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit time\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import (\n",
    "    LinearDiscriminantAnalysis,\n",
    "    QuadraticDiscriminantAnalysis,\n",
    ")\n",
    "\n",
    "from sklearn.feature_extraction.text import (\n",
    "    CountVectorizer, \n",
    "    TfidfTransformer, \n",
    "    TfidfVectorizer\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, \n",
    "    GridSearchCV, \n",
    "    KFold, \n",
    "    cross_val_score\n",
    ")\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    accuracy_score, \n",
    "    confusion_matrix\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d5c2697",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/MLUnige2021_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b1b7cf",
   "metadata": {},
   "source": [
    "## 2. Strategy\n",
    "\n",
    "1. Preprocess the text\n",
    "    1. remove punctuation marks\n",
    "    2. remove stopwords (en)\n",
    "    3. stem or lemmatize the words\n",
    "    \n",
    "    \n",
    "2. take a sample of our whole dataset (200k?) to do our preliminary test. We can't do cross validation on the whole dataset.\n",
    "\n",
    "3. Begin to fit the models. \n",
    "    1. Pipeline with TfidfTransformer (or the other one I don't remember the name)\n",
    "    2. BerNB\n",
    "    3. LogisticRegression\n",
    "    4. RidgeClassifier\n",
    "    5. SGDClassifier\n",
    "    6. SVC\n",
    "    7. RandomForestClassifier\n",
    "    8. DecisionTreeClassifier\n",
    "    9. KNeighborsClassifier\n",
    "    10. LDA? QDA?\n",
    "4. Select a few of the best models, CV with bigger dataset\n",
    "5. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874e9acf",
   "metadata": {},
   "source": [
    "# 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89eddff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanitized: hello shiva marilyn corona \n"
     ]
    }
   ],
   "source": [
    "# Some special strings to test \n",
    "txt1 = \"Hello @Shiva and @Marilyn! https://hello.ch 12334 #corona ...\"\n",
    "print(\"Sanitized:\", sanitize(txt1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d411db2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                          | 569/1280000 [00:00<03:45, 5675.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pickle file found, sanitizing existing df\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 1280000/1280000 [02:38<00:00, 8056.71it/s]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df_san = pd.read_pickle(\"./data/sanitized2.pkl\")\n",
    "except FileNotFoundError:\n",
    "    print(\"No pickle file found, sanitizing existing df\")\n",
    "    \n",
    "    # Sanitize whole dataset\n",
    "    df_san = df.copy()\n",
    "    df_san[\"sanitized\"] = df[\"text\"].progress_apply(sanitize)\n",
    "\n",
    "    # Export it to pickle so we don't have to redo it\n",
    "    df_san.to_pickle(\"./data/sanitized2.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48f88566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1280000, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>emotion</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>date</th>\n",
       "      <th>lyx_query</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>sanitized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2063391019</td>\n",
       "      <td>Sun Jun 07 02:28:13 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>BerryGurus</td>\n",
       "      <td>@BreeMe more time to play with you BlackBerry ...</td>\n",
       "      <td>breem time play blackberri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2000525676</td>\n",
       "      <td>Mon Jun 01 22:18:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>peterlanoie</td>\n",
       "      <td>Failed attempt at booting to a flash drive. Th...</td>\n",
       "      <td>fail attempt boot flash drive fail attempt swi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2218180611</td>\n",
       "      <td>Wed Jun 17 22:01:38 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>will_tooker</td>\n",
       "      <td>@msproductions Well ain't that the truth. Wher...</td>\n",
       "      <td>msproduct well truth damn auto lock disabl go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2190269101</td>\n",
       "      <td>Tue Jun 16 02:14:47 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>sammutimer</td>\n",
       "      <td>@Meaghery cheers Craig - that was really sweet...</td>\n",
       "      <td>meagheri cheer craig realli sweet repli pump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2069249490</td>\n",
       "      <td>Sun Jun 07 15:31:58 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ohaijustin</td>\n",
       "      <td>I was reading the tweets that got send to me w...</td>\n",
       "      <td>read tweet got send lie phone face drop amp hi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  emotion    tweet_id                          date lyx_query  \\\n",
       "0   0        1  2063391019  Sun Jun 07 02:28:13 PDT 2009  NO_QUERY   \n",
       "1   1        0  2000525676  Mon Jun 01 22:18:53 PDT 2009  NO_QUERY   \n",
       "2   2        0  2218180611  Wed Jun 17 22:01:38 PDT 2009  NO_QUERY   \n",
       "3   3        1  2190269101  Tue Jun 16 02:14:47 PDT 2009  NO_QUERY   \n",
       "4   4        0  2069249490  Sun Jun 07 15:31:58 PDT 2009  NO_QUERY   \n",
       "\n",
       "          user                                               text  \\\n",
       "0   BerryGurus  @BreeMe more time to play with you BlackBerry ...   \n",
       "1  peterlanoie  Failed attempt at booting to a flash drive. Th...   \n",
       "2  will_tooker  @msproductions Well ain't that the truth. Wher...   \n",
       "3   sammutimer  @Meaghery cheers Craig - that was really sweet...   \n",
       "4   ohaijustin  I was reading the tweets that got send to me w...   \n",
       "\n",
       "                                           sanitized  \n",
       "0                         breem time play blackberri  \n",
       "1  fail attempt boot flash drive fail attempt swi...  \n",
       "2   msproduct well truth damn auto lock disabl go...  \n",
       "3      meagheri cheer craig realli sweet repli pump   \n",
       "4  read tweet got send lie phone face drop amp hi...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_san.shape)\n",
    "df_san.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04615f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before sanitizing 18398298\n",
      "After sanitizing 10730388\n"
     ]
    }
   ],
   "source": [
    "print(\"Before sanitizing\", df['text'].apply(lambda x: len(x.split(' '))).sum())\n",
    "print(\"After sanitizing\", df_san['sanitized'].apply(lambda x: len(x.split(' '))).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226794b9",
   "metadata": {},
   "source": [
    "Just to check, we see that before sanitizing, we had 18'398'298 words. We were able to halve it to 9'575'942 by sanitization and stemming our tweets.\n",
    "\n",
    "Before fitting our models, we also take a subsample to be able to compute them faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1aecae3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Id 423388\n",
      "Last Id 63949\n",
      "Length 128000\n"
     ]
    }
   ],
   "source": [
    "# Preprocessed by me\n",
    "df_sub_san = df_san.sample(frac=0.1, random_state=SEED)\n",
    "\n",
    "# To check reproducibility\n",
    "print(\"First Id\", df_sub_san[\"Id\"].iloc[0])\n",
    "print(\"Last Id\", df_sub_san[\"Id\"].iloc[-1])\n",
    "print(\"Length\", df_sub_san.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e5fbc51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Id 423388\n",
      "Last Id 63949\n",
      "Length 128000\n"
     ]
    }
   ],
   "source": [
    "#No preprocess, will use default by scikit\n",
    "df_sub_nosan = df.sample(frac=0.1, random_state=SEED)\n",
    "\n",
    "# To check reproducibility\n",
    "print(\"First Id\", df_sub_nosan[\"Id\"].iloc[0])\n",
    "print(\"Last Id\", df_sub_nosan[\"Id\"].iloc[-1])\n",
    "print(\"Length\", df_sub_nosan.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9587eb",
   "metadata": {},
   "source": [
    "# 2. Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9969c0f",
   "metadata": {},
   "source": [
    "## 1. with manual preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b793a8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split, manual preprocessing\n",
    "#X_train, X_test, y_train, y_test = train_test_split(df_sub_san[\"sanitized\"], df_sub_san[\"emotion\"], \n",
    "#                                                    test_size=0.2, shuffle=True, random_state=SEED)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_sub_nosan[\"text\"], df_sub_nosan[\"emotion\"], \n",
    "                                                    test_size=0.2, shuffle=True, random_state=SEED)\n",
    "\n",
    "#only 4 folds because I have 4 cores, just to test\n",
    "folds = KFold(n_splits=4, shuffle=True, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ab1efd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  (102400,)\n",
      "X_test:  (25600,)\n",
      "y_train:  (102400,)\n",
      "y_test:  (25600,)\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "print(\"X_train: \", X_train.shape)\n",
    "print(\"X_test: \", X_test.shape)\n",
    "print(\"y_train: \", y_train.shape)\n",
    "print(\"y_test: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2d065f",
   "metadata": {},
   "source": [
    "First model we will try is the `BernoulliNB` since we have binary data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a6ea1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 4.277349948883057\n",
      "Mean CV accuracy: 0.745966796875\n",
      "Test accuracy: 0.7516015625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nWhole dataset:\\nTime 21.568854093551636\\nMean CV accuracy: 0.7639306640625\\nTest accuracy: 0.76375390625\\n\\n10% sample:\\nTime 2.146036386489868\\nMean CV accuracy: 0.7483203125\\nTest accuracy: 0.7523828125\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "berNB = Pipeline(\n",
    "    [\n",
    "        (\"tfidf\", TfidfVectorizer()),\n",
    "        (\"clf\", BernoulliNB()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "CV_ber = cross_val_score(\n",
    "    berNB, X_train, y_train, scoring=\"accuracy\", cv=folds, n_jobs=-1\n",
    ")\n",
    "\n",
    "berNB.fit(X_train, y_train)\n",
    "y_pred = berNB.predict(X_test)\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "print(f\"Time {time.time() - start}\")\n",
    "print(f\"Mean CV accuracy: {np.mean(CV_ber)}\")\n",
    "print(f\"Test accuracy: {score}\")\n",
    "\n",
    "\"\"\"\n",
    "Whole dataset:\n",
    "Time 21.568854093551636\n",
    "Mean CV accuracy: 0.7639306640625\n",
    "Test accuracy: 0.76375390625\n",
    "\n",
    "10% sample:\n",
    "Time 2.146036386489868\n",
    "Mean CV accuracy: 0.7483203125\n",
    "Test accuracy: 0.7523828125\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0d6cbde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models = [LogisticRegression(warm_start=True)]\n",
    "\n",
    "params_tfid = {\n",
    "    \"tfidfvectorizer__strip_accents\": [\"unicode\"],\n",
    "    \"tfidfvectorizer__lowercase\": [True],\n",
    "    \"tfidfvectorizer__stop_words\": [\"english\"],\n",
    "    \"tfidfvectorizer__norm\": [\"l2\"],\n",
    "    \"tfidfvectorizer__analyzer\": [\"word\"],\n",
    "    \"tfidfvectorizer__use_idf\": [True],\n",
    "    \"tfidfvectorizer__smooth_idf\": [True, False],\n",
    "    \"tfidfvectorizer__sublinear_tf\": [True, False]\n",
    "}\n",
    "\n",
    "params = {\n",
    "    \"bernoullinb\": {\n",
    "        \"bernoullinb__alpha\": [7.5],\n",
    "        \"bernoullinb__fit_prior\": [False, True],\n",
    "    },\n",
    "    \"ridgeclassifier\": {\n",
    "        \"ridgeclassifier__alpha\": np.linspace(1e-5, 10, 5),\n",
    "        \"ridgeclassifier__class_weight\": [\"balanced\", None],\n",
    "        \"ridgeclassifier__normalize\": [False, True],\n",
    "        \n",
    "    },\n",
    "    \"logisticregression\": {\n",
    "        #\"logisticregression__penalty\": [\"l1\", \"l2\", \"elasticnet\"],\n",
    "        #\"logisticregression__penalty\": [\"l1\"],\n",
    "        #\"logisticregression__dual\": [False, True], try with liblinear\n",
    "        \"logisticregression__C\": [1.2],\n",
    "        #\"logisticregression__C\": [1e-1],\n",
    "        #\"logisticregression__random_state\": [SEED],\n",
    "        #\"logisticregression__solver\": [\"newton-cg\", \"lbfgs\", \"saga\"],\n",
    "        #\"logisticregression__solver\": [\"saga\"],\n",
    "        #\"logisticregression__l1_ratio\": np.linspace(0.1, 0.9, 5),\n",
    "    },\n",
    "    \"sgdclassifier\": {\n",
    "        \"sgdclassifier__random_state\": [SEED],\n",
    "        \"sgdclassifier__loss\": [\"modified_huber\"],\n",
    "        \"sgdclassifier__alpha\": 10**np.linspace(-3, -0.001, 4),\n",
    "    },\n",
    "    \"baggingclassifier\": {\n",
    "        \"baggingclassifier__random_state\": [SEED],\n",
    "        \"baggingclassifier__n_estimators\": [30],\n",
    "        \"baggingclassifier__max_samples\": [0.05],\n",
    "        \"baggingclassifier__max_features\": [0.5],\n",
    "        \n",
    "    },\n",
    "    \"randomforestclassifier\": {\n",
    "        \"randomforestclassifier__random_state\": [SEED],\n",
    "    },\n",
    "    \"svc\": {\n",
    "        \"svc__random_state\": [SEED],\n",
    "    },\n",
    "    \"linearsvc\": {\n",
    "        \"linearsvc__random_state\": [SEED],\n",
    "        \"linearsvc__loss\": [\"squared_hinge\"],\n",
    "        \"linearsvc__penalty\": [\"l2\"],\n",
    "        \"linearsvc__max_iter\": [1000],\n",
    "        \"linearsvc__dual\": [False],\n",
    "        \"linearsvc__C\": np.linspace(0.01, 0.2, 10),\n",
    "        \"linearsvc__class_weight\": [\"balanced\"],\n",
    "    }\n",
    "}\n",
    "\n",
    "# If we also want to gridsearch the different Tfidf params\n",
    "for k, v in params_tfid.items():\n",
    "    #params[\"bernoullinb\"][k] = v\n",
    "    params[\"logisticregression\"][k] = v\n",
    "    #Easier if we comment above\n",
    "    pass\n",
    "\n",
    "pipes = []\n",
    "\n",
    "# Also check what we can do with the TfidfVectorizer parameters\n",
    "for model in models:\n",
    "    pipe = make_pipeline(TfidfVectorizer(), model)\n",
    "    pipes.append(pipe)\n",
    "    \n",
    "    # Will use that once we have the best params\n",
    "    #pipe.set_params(**params[pipe.steps[1][0]])\n",
    "\n",
    "# Initialize empty dictionary\n",
    "reports = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3c13c6cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00000000e-03, 6.67333333e-01, 1.33366667e+00, 2.00000000e+00])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linspace(1e-3, 2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "89f20c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logisticregression\n",
      "Fitting 4 folds for each of 4 candidates, totalling 16 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olivi\\anaconda3\\envs\\env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 23.103020668029785s\n",
      "Test accuracy: 0.760703125\n"
     ]
    }
   ],
   "source": [
    "# Fit each different pipeline\n",
    "\n",
    "for pipe in pipes:\n",
    "    print(pipe.steps[1][0])\n",
    "    start = time.time()\n",
    "    \n",
    "    gridsearch = GridSearchCV(pipe, params[pipe.steps[1][0]], scoring=\"accuracy\", cv=folds, n_jobs=-1, verbose=3)\n",
    "    gridsearch.fit(X_train, y_train)\n",
    "    y_pred = gridsearch.predict(X_test)\n",
    "    \n",
    "    score = accuracy_score(y_test, y_pred)\n",
    "    resdf = pd.DataFrame(gridsearch.cv_results_)\n",
    "    \n",
    "    reports[pipe.steps[1][0]] = classification_report(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Time {time.time() - start}s\")\n",
    "    #print(resdf[resdf[\"rank_test_score\"] == 1])\n",
    "    print(f\"Test accuracy: {score}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "95f458f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_logisticregression__C</th>\n",
       "      <th>param_tfidfvectorizer__analyzer</th>\n",
       "      <th>param_tfidfvectorizer__lowercase</th>\n",
       "      <th>param_tfidfvectorizer__norm</th>\n",
       "      <th>param_tfidfvectorizer__stop_words</th>\n",
       "      <th>param_tfidfvectorizer__strip_accents</th>\n",
       "      <th>param_tfidfvectorizer__use_idf</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.414015</td>\n",
       "      <td>0.320567</td>\n",
       "      <td>0.337937</td>\n",
       "      <td>0.014908</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>word</td>\n",
       "      <td>True</td>\n",
       "      <td>l2</td>\n",
       "      <td>english</td>\n",
       "      <td>unicode</td>\n",
       "      <td>True</td>\n",
       "      <td>{'logisticregression__C': 1.3333333333333333, ...</td>\n",
       "      <td>0.758125</td>\n",
       "      <td>0.755625</td>\n",
       "      <td>0.759609</td>\n",
       "      <td>0.761367</td>\n",
       "      <td>0.758682</td>\n",
       "      <td>0.002105</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.248127</td>\n",
       "      <td>0.479268</td>\n",
       "      <td>0.382911</td>\n",
       "      <td>0.034352</td>\n",
       "      <td>1.0</td>\n",
       "      <td>word</td>\n",
       "      <td>True</td>\n",
       "      <td>l2</td>\n",
       "      <td>english</td>\n",
       "      <td>unicode</td>\n",
       "      <td>True</td>\n",
       "      <td>{'logisticregression__C': 1.0, 'tfidfvectorize...</td>\n",
       "      <td>0.758867</td>\n",
       "      <td>0.755195</td>\n",
       "      <td>0.759297</td>\n",
       "      <td>0.761289</td>\n",
       "      <td>0.758662</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.240951</td>\n",
       "      <td>0.390944</td>\n",
       "      <td>0.335812</td>\n",
       "      <td>0.019201</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>word</td>\n",
       "      <td>True</td>\n",
       "      <td>l2</td>\n",
       "      <td>english</td>\n",
       "      <td>unicode</td>\n",
       "      <td>True</td>\n",
       "      <td>{'logisticregression__C': 1.6666666666666665, ...</td>\n",
       "      <td>0.757070</td>\n",
       "      <td>0.755313</td>\n",
       "      <td>0.759648</td>\n",
       "      <td>0.760781</td>\n",
       "      <td>0.758203</td>\n",
       "      <td>0.002143</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.458458</td>\n",
       "      <td>0.253091</td>\n",
       "      <td>0.353544</td>\n",
       "      <td>0.020942</td>\n",
       "      <td>2.0</td>\n",
       "      <td>word</td>\n",
       "      <td>True</td>\n",
       "      <td>l2</td>\n",
       "      <td>english</td>\n",
       "      <td>unicode</td>\n",
       "      <td>True</td>\n",
       "      <td>{'logisticregression__C': 2.0, 'tfidfvectorize...</td>\n",
       "      <td>0.757422</td>\n",
       "      <td>0.755000</td>\n",
       "      <td>0.759180</td>\n",
       "      <td>0.761094</td>\n",
       "      <td>0.758174</td>\n",
       "      <td>0.002246</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.569561</td>\n",
       "      <td>0.123064</td>\n",
       "      <td>0.340913</td>\n",
       "      <td>0.018221</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>word</td>\n",
       "      <td>True</td>\n",
       "      <td>l2</td>\n",
       "      <td>english</td>\n",
       "      <td>unicode</td>\n",
       "      <td>False</td>\n",
       "      <td>{'logisticregression__C': 1.6666666666666665, ...</td>\n",
       "      <td>0.758750</td>\n",
       "      <td>0.754258</td>\n",
       "      <td>0.759102</td>\n",
       "      <td>0.759687</td>\n",
       "      <td>0.757949</td>\n",
       "      <td>0.002157</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.582193</td>\n",
       "      <td>0.124175</td>\n",
       "      <td>0.343528</td>\n",
       "      <td>0.012095</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>word</td>\n",
       "      <td>True</td>\n",
       "      <td>l2</td>\n",
       "      <td>english</td>\n",
       "      <td>unicode</td>\n",
       "      <td>False</td>\n",
       "      <td>{'logisticregression__C': 1.3333333333333333, ...</td>\n",
       "      <td>0.758203</td>\n",
       "      <td>0.753828</td>\n",
       "      <td>0.758867</td>\n",
       "      <td>0.759492</td>\n",
       "      <td>0.757598</td>\n",
       "      <td>0.002224</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.328222</td>\n",
       "      <td>0.142667</td>\n",
       "      <td>0.308464</td>\n",
       "      <td>0.031369</td>\n",
       "      <td>2.0</td>\n",
       "      <td>word</td>\n",
       "      <td>True</td>\n",
       "      <td>l2</td>\n",
       "      <td>english</td>\n",
       "      <td>unicode</td>\n",
       "      <td>False</td>\n",
       "      <td>{'logisticregression__C': 2.0, 'tfidfvectorize...</td>\n",
       "      <td>0.758008</td>\n",
       "      <td>0.752969</td>\n",
       "      <td>0.759375</td>\n",
       "      <td>0.759766</td>\n",
       "      <td>0.757529</td>\n",
       "      <td>0.002713</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.908550</td>\n",
       "      <td>0.179291</td>\n",
       "      <td>0.370534</td>\n",
       "      <td>0.029413</td>\n",
       "      <td>1.0</td>\n",
       "      <td>word</td>\n",
       "      <td>True</td>\n",
       "      <td>l2</td>\n",
       "      <td>english</td>\n",
       "      <td>unicode</td>\n",
       "      <td>False</td>\n",
       "      <td>{'logisticregression__C': 1.0, 'tfidfvectorize...</td>\n",
       "      <td>0.757578</td>\n",
       "      <td>0.753828</td>\n",
       "      <td>0.758008</td>\n",
       "      <td>0.759258</td>\n",
       "      <td>0.757168</td>\n",
       "      <td>0.002025</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "2       3.414015      0.320567         0.337937        0.014908   \n",
       "0       3.248127      0.479268         0.382911        0.034352   \n",
       "4       3.240951      0.390944         0.335812        0.019201   \n",
       "6       3.458458      0.253091         0.353544        0.020942   \n",
       "5       3.569561      0.123064         0.340913        0.018221   \n",
       "3       3.582193      0.124175         0.343528        0.012095   \n",
       "7       3.328222      0.142667         0.308464        0.031369   \n",
       "1       3.908550      0.179291         0.370534        0.029413   \n",
       "\n",
       "  param_logisticregression__C param_tfidfvectorizer__analyzer  \\\n",
       "2                    1.333333                            word   \n",
       "0                         1.0                            word   \n",
       "4                    1.666667                            word   \n",
       "6                         2.0                            word   \n",
       "5                    1.666667                            word   \n",
       "3                    1.333333                            word   \n",
       "7                         2.0                            word   \n",
       "1                         1.0                            word   \n",
       "\n",
       "  param_tfidfvectorizer__lowercase param_tfidfvectorizer__norm  \\\n",
       "2                             True                          l2   \n",
       "0                             True                          l2   \n",
       "4                             True                          l2   \n",
       "6                             True                          l2   \n",
       "5                             True                          l2   \n",
       "3                             True                          l2   \n",
       "7                             True                          l2   \n",
       "1                             True                          l2   \n",
       "\n",
       "  param_tfidfvectorizer__stop_words param_tfidfvectorizer__strip_accents  \\\n",
       "2                           english                              unicode   \n",
       "0                           english                              unicode   \n",
       "4                           english                              unicode   \n",
       "6                           english                              unicode   \n",
       "5                           english                              unicode   \n",
       "3                           english                              unicode   \n",
       "7                           english                              unicode   \n",
       "1                           english                              unicode   \n",
       "\n",
       "  param_tfidfvectorizer__use_idf  \\\n",
       "2                           True   \n",
       "0                           True   \n",
       "4                           True   \n",
       "6                           True   \n",
       "5                          False   \n",
       "3                          False   \n",
       "7                          False   \n",
       "1                          False   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "2  {'logisticregression__C': 1.3333333333333333, ...           0.758125   \n",
       "0  {'logisticregression__C': 1.0, 'tfidfvectorize...           0.758867   \n",
       "4  {'logisticregression__C': 1.6666666666666665, ...           0.757070   \n",
       "6  {'logisticregression__C': 2.0, 'tfidfvectorize...           0.757422   \n",
       "5  {'logisticregression__C': 1.6666666666666665, ...           0.758750   \n",
       "3  {'logisticregression__C': 1.3333333333333333, ...           0.758203   \n",
       "7  {'logisticregression__C': 2.0, 'tfidfvectorize...           0.758008   \n",
       "1  {'logisticregression__C': 1.0, 'tfidfvectorize...           0.757578   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  mean_test_score  \\\n",
       "2           0.755625           0.759609           0.761367         0.758682   \n",
       "0           0.755195           0.759297           0.761289         0.758662   \n",
       "4           0.755313           0.759648           0.760781         0.758203   \n",
       "6           0.755000           0.759180           0.761094         0.758174   \n",
       "5           0.754258           0.759102           0.759687         0.757949   \n",
       "3           0.753828           0.758867           0.759492         0.757598   \n",
       "7           0.752969           0.759375           0.759766         0.757529   \n",
       "1           0.753828           0.758008           0.759258         0.757168   \n",
       "\n",
       "   std_test_score  rank_test_score  \n",
       "2        0.002105                1  \n",
       "0        0.002200                2  \n",
       "4        0.002143                3  \n",
       "6        0.002246                4  \n",
       "5        0.002157                5  \n",
       "3        0.002224                6  \n",
       "7        0.002713                7  \n",
       "1        0.002025                8  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resdf.sort_values(by=[\"rank_test_score\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4950dd56",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "With less sanitization (either manual or scikit preprocess)\n",
    "\n",
    "Linear models: <br>\n",
    "BernoulliNB: <br>\n",
    "0.76 <br>\n",
    "LogisticRegression: <br>\n",
    "0.76 <br>\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c4041e",
   "metadata": {},
   "source": [
    "No matter the classifier, we don't seem to go above 76%. Look into the data to see if we can see something.\n",
    "-> delete very short tweets?\n",
    "-> lemmatization?\n",
    "\n",
    "Maybe deleted too much noise during preprocessing, only remove stopwords? (+URLs?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bae9c0",
   "metadata": {},
   "source": [
    "According to internet:\n",
    "\"Also, very short texts are likely to have noisy tf–idf values while the binary occurrence info is more stable.\" play with the binary parameter of CountVectorizer!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
